{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Node(Object):\\n    def __init__(self):\\n        \\n    def \\n'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class NN():\n",
    "    \"\"\"\n",
    "    Class for a neural network. Requires input/output sizes, number of hidden layers, and number of neurons\n",
    "    at each layer (we assume all hidden layers are of the same size)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, num_HL, hidden_size, output_size):\n",
    "        # Initialize by setting random \n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = num_HL\n",
    "        # NOTE we are assuming all hidden layers are the same size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Activations for each neuron\n",
    "        # + 1 for bias neuron\n",
    "        self.activations_in = np.ones(self.input_size + 1)\n",
    "        # Hidden can comprise multiple layers, so we have a matrix\n",
    "        # + 1 for bias neuron\n",
    "        self.activations_hidden = np.ones((self.hidden_layers, self.hidden_size + 1))\n",
    "        self.activations_out = np.ones(self.output_size)\n",
    "        # Weights of all the edges, randomized for good results\n",
    "        # PLUS ONE FOR BIASES\n",
    "        self.weights_in = np.random.randn(self.input_size + 1, self.hidden_size)\n",
    "        self.weights_in = np.column_stack((self.weights_in, np.zeros(self.input_size + 1)))\n",
    "        # We will only have hidden weights if there are multiple hidden layers\n",
    "        if self.hidden_layers > 1:\n",
    "            self.weights_hidden = np.random.randn(self.hidden_layers - 1, self.hidden_size + 1, self.hidden_size + 1)\n",
    "            # Set the weights corresponding with next layers biases to 0\n",
    "            for weights in self.weights_hidden:\n",
    "                weights[-1] = 0.0\n",
    "        else:\n",
    "            self.weights_hidden = []\n",
    "        # No plus one for output, as it should not have a bias parameter\n",
    "        self.weights_out = np.random.randn(self.hidden_size + 1, self.output_size)\n",
    "        # To be valued when train() is called\n",
    "        self.learning_rate = 0.0\n",
    "\n",
    "        # Instantiate deltas for holding gradients\n",
    "        self.deltas_in = []\n",
    "        self.Deltas_hidden = []\n",
    "        self.deltas_out = []\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Sigmoid function for calculating a distribution over 2 classes\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def _derivative_sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Derivative of the sigmoid function where x = the output of the sigmoid\n",
    "        \n",
    "        This can be used in backpropogation, wherein we would have \n",
    "        already computed the sigmoid in the forward pass, and we can draw upon its cached value\n",
    "        \"\"\"\n",
    "        return self._sigmoid(x) * (1.0 - self._sigmoid(x))\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        exponentials = [np.exp(p) for p in x]\n",
    "        denominator = sum(exponentials)\n",
    "        return [p / denominator for p in exponentials]\n",
    "    \n",
    "    def _relu(self, x):\n",
    "        \"\"\"\n",
    "        relu function used for activation\n",
    "        \"\"\"\n",
    "        return max(x, 0.0)\n",
    "    \n",
    "    def _derivative_relu(self, x):\n",
    "        \"\"\"\n",
    "        Derivative of the relu function, the input will be the output of the relu function.\n",
    "        This is because in practice we will have already performed this computation in the forward pass\n",
    "        so in the backward pass, we need to find its derivative drawing upon the cached relu(x).\n",
    "        \"\"\"\n",
    "        return 1 if x > 0.0 else 0.0\n",
    "    \n",
    "    def _binary_cross_ent(self, y_hat, y):\n",
    "        \"\"\"\n",
    "        This basically finds the negative of the log probability of class1 - its inverse\n",
    "        \"\"\"\n",
    "        return (-y * np.log(y_hat)) - ((1 - y) * np.log(1 - y_hat))\n",
    "    \n",
    "    def _negative_log_likelihood(self, y_hat):\n",
    "        return -np.log(y_hat)\n",
    "    \n",
    "    def _derivative_negative_log_likelihood(self, y_hat):\n",
    "        return -1/y_hat\n",
    "    \n",
    "    def _derivative_binary_cross_ent(self, y_hat, y):\n",
    "        \"\"\"\n",
    "        Derivative of binary cross-entropy\n",
    "        \n",
    "        This description is misleading. \n",
    "        This is the part of the partial derivative of binary cross-entropy \n",
    "        w.r.t the parameters of our function. In practice, the other part is \n",
    "        the dot product of this and the activations (activate(w, x))\n",
    "        \"\"\"\n",
    "        #return -(y / y_hat) - ((1 - y) / (1 - y_hat))\n",
    "        return (y_hat - y)\n",
    "\n",
    "    def _activate(self, x):\n",
    "        \"\"\"\n",
    "        RELU for non-linear activation function\n",
    "        \"\"\"\n",
    "        return self._relu(x)\n",
    "    \n",
    "    def _activate_vector(self, X):\n",
    "        \"\"\"\n",
    "        Run on a numpy vector\n",
    "        \"\"\"\n",
    "        activations = np.vectorize(self._activate)\n",
    "        return activations(X)\n",
    "    \n",
    "    def _derivative_activation(self, x):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the activation function given the activation output\n",
    "        \n",
    "        x: activate(node)\n",
    "        \"\"\"\n",
    "        return self._derivative_relu(x)\n",
    "    \n",
    "    def _derivative_vector_activation(self, X):\n",
    "        \"\"\"\n",
    "        Derivative for each scalar in a numpy vector\n",
    "        \"\"\"\n",
    "        derivative_activations = np.vectorize(self._derivative_activation)\n",
    "        return derivative_activations(X)\n",
    "\n",
    "    def _loss(self, y_hat, y):\n",
    "        \"\"\"\n",
    "        y_hat: sofmax vector\n",
    "        y:     one-hot vector for the target\n",
    "        \n",
    "        Here we will plug in the negative_log_likelihood\n",
    "        in order to be able to compare the proability of our output at\n",
    "        the correct class, to 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Get the index of the correct class \n",
    "        (numpy will return a tuple of the index in each dimension)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the first (and only) dim, and the first (and only) index\n",
    "        i = np.where(y==1)[0][0]\n",
    "        return self._negative_log_likelihood(y_hat[i])\n",
    "    \n",
    "    def _derivative_loss(self, y_hat, y):\n",
    "        \"\"\"\n",
    "        This will be used in backprop for finding L'(output_layer_node)\n",
    "        \"\"\"\n",
    "        # Get the first (and only) dim, and the first (and only) index\n",
    "        i = np.where(y==1)[0][0]\n",
    "        return self._derivative_negative_log_likelihood(y_hat[i])\n",
    "    \n",
    "    def _targets_to_one_hots(self, targets):\n",
    "        \"\"\"\n",
    "        Interpret a vector of targets into a matrix\n",
    "        of one-hot representations\n",
    "        \"\"\"\n",
    "        # Get the number of unique target classes\n",
    "        num_classes = len(set(targets))\n",
    "        # Instantiate a matrix of one-hot vectors\n",
    "        # with one row per target, and one col per class\n",
    "        one_hots = np.zeros((len(targets), num_classes))\n",
    "        for i, one_hot in enumerate(one_hots):\n",
    "            # Set the one-hot vector to hae a 1 at its corresponding target slot\n",
    "            t = targets[i]\n",
    "            one_hot[t] = 1\n",
    "            \n",
    "        return one_hots\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass: Calculate the activations of each neuron\n",
    "        \"\"\"\n",
    "        if len(inputs) != self.input_size:\n",
    "          raise Exception(\"That is not the size of the input layer... try %i\" % self.input_size)\n",
    "        \n",
    "        # Set input activations, no need to actually calculate anything\n",
    "        for i, input in enumerate(inputs):\n",
    "            self.activations_in[i] = input\n",
    "        \n",
    "        # calculate the activations for each hidden layer\n",
    "        for h_layer_i in range(self.hidden_layers):\n",
    "            # Need to take previous layer activation value * weights for a given layer\n",
    "            # Starting with input layer X first hidden layer\n",
    "            if h_layer_i == 0:\n",
    "                \"\"\"\n",
    "                # Loop over the layer\n",
    "                for h_dim_j in range(self.hidden_size):\n",
    "                    # Loop over neurons in the layer before it\n",
    "                    for k in range(self.input_size):\n",
    "                        # Sum [f_k * w_k_j for k in input layer]\n",
    "                        self.activations_hidden[h_layer_i][h_dim_j] += self.activations_in[k] * self.weights_in[k][h_dim_j]\n",
    "                    # h(sum from above), aka run the nonlinear activation function\n",
    "                    self.activations_hidden[h_layer_i][h_dim_j] = self._activate(self.activations_hidden[h_layer_i][h_dim_j])\n",
    "                \"\"\"\n",
    "                # multiply the previous layer's activations by its weight vector for this layer's activations\n",
    "                self.activations_hidden[h_layer_i] = self.activations_in.T.dot(self.weights_in)\n",
    "                # Reset bias activation to 1.0\n",
    "                self.activations_hidden[h_layer_i][-1] = 1.0\n",
    "            else:\n",
    "                \"\"\"\n",
    "                if h_layer_i == 0:\n",
    "                    # Loop over the layer\n",
    "                    for h_dim_j in range(self.hidden_size):\n",
    "                        # Loop over neurons in the layer before it\n",
    "                        for k in range(self.hidden_size):\n",
    "                            # Sum [f_k * w_k_j for k in previous hidden layer]\n",
    "                            self.activations_hidden[h_layer_i][h_dim_j] += self.activations_hidden[h_layer_i - 1][k]\\\n",
    "                            * self.weights_hidden[h_layer_i][k][h_dim_j]\n",
    "                        # h(sum from above), aka run the nonlinear activation function\n",
    "                        self.activations_hidden[h_layer_i][h_dim_j] = self.activations_hidden[h_layer_i][h_dim_j]\n",
    "                \"\"\"\n",
    "                # multiply the previous layer's activations by its weight vector for this layer's activations\n",
    "                self.activations_hidden[h_layer_i] = self._activate_vector(self.activations_hidden[h_layer_i - 1]).T.dot(self.weights_hidden[h_layer_i - 1])\n",
    "                # Reset bias activation to 1.0\n",
    "                self.activations_hidden[h_layer_i][-1] = 1.0\n",
    "\n",
    "                \n",
    "        # Output activations will be the dot product of the final hidden layer, and the output weights\n",
    "        \"\"\"\n",
    "        for i in range(self.output_size):\n",
    "            for j in range(self.hidden_size):\n",
    "                print(self.weights_out)\n",
    "                print(self.weights_out[j][i])\n",
    "                self.activations_out[i] += self.activations_hidden[-1][j] * self.weights_out[j][i]\n",
    "            # h(sum from above), aka run the nonlinear activation function\n",
    "            self.activations_out[i] = self._activate(self.activations_out[i])\n",
    "        \"\"\"\n",
    "        # Activate the vector before, but do not activate the activations_out\n",
    "        self.activations_out = self._activate(self.activations_hidden[-1]).T.dot(self.weights_out)\n",
    "        # [:-1] because this one should not have a bias parameter\n",
    "        #self.activations_out = self._activate_vector(self.activations_out)\n",
    "        \n",
    "        #Print all of the weights, to see updates\n",
    "        \"\"\"\n",
    "        print(\"ACTIVATIONS\")\n",
    "        print(self.activations_in)\n",
    "        print(self.activations_hidden)\n",
    "        print(self.activations_out)\n",
    "        \"\"\"\n",
    "        print(\"WEIGHTS:\")\n",
    "        print(self.weights_in)\n",
    "        print(self.weights_hidden)\n",
    "        print(self.weights_out)\n",
    "\n",
    "    def backward(self, targets):\n",
    "        \"\"\"\n",
    "        Backpropogation for finding the partial derivative of the each node w.r.t the loss function,\n",
    "        and updating weights based on those gradients\n",
    "        \"\"\"\n",
    "        if len(targets) != len(self.activations_out):\n",
    "            raise Exception(\"Your labels are not the same size as your output layer!\")\n",
    "        # Calculate loss - there will be a value for each node in the output layer\n",
    "        # Take the simoid of the activations of the output layer, because we are doing 2 class classification\n",
    "        # ***If we have >2 classes, we would use softmax***\n",
    "        \"\"\"\n",
    "        print(\"ACTIVATIONS IN\")\n",
    "        print(self.activations_in)\n",
    "        \n",
    "        print(\"ACTIVATIONS HIDDEN\")\n",
    "        print(self.activations_hidden)\n",
    "        \n",
    "        print(\"ACTIVATIONS OUT\")\n",
    "        print(self.activations_out)\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"SOFTMAX_LAYER\")\n",
    "        print(self._softmax(self.activations_out))\n",
    "        \"\"\"\n",
    "        print(\"TARGETS\")\n",
    "        print(targets)\n",
    "        \"\"\"\n",
    "        loss = self._loss(self._softmax(self.activations_out), targets)\n",
    "        \n",
    "        \"\"\"\n",
    "        Now we need to calculate the partial derivative of the loss w.r.t each weight.\n",
    "        Think of this as finding the amount that each node contributes to a change in the final loss.\n",
    "        \n",
    "        Each node has a value \"delta\", which represents the partial derivative of the loss w.r.t. its value:\n",
    "        Use the partial derivative of the loss function, in our case binary cross-entropy\n",
    "        \"\"\"\n",
    "        self.deltas_out = np.zeros([self.output_size])\n",
    "        derivative_loss = self._derivative_loss(self._softmax(self.activations_out), targets)\n",
    "        print(\"LOSS PER INSTANCE: %2f\" % loss)\n",
    "        print(\"DERIVATIVE LOSS: %2f\" % derivative_loss)\n",
    "        print(\"DERIVATIVE ACTIVATIONS:\")\n",
    "        print(self._derivative_vector_activation(self.activations_out))\n",
    "        for i, activation_out in enumerate(self.activations_out):\n",
    "            self.deltas_out[i] = derivative_loss * self._derivative_activation(activation_out)\n",
    "        \n",
    "        \"\"\"\n",
    "        Find derivative of activation (activation was found in the forward pass) * derivative of the inner function,\n",
    "        which is the parameter w\n",
    "        \"\"\"\n",
    "        self.Deltas_hidden = np.zeros([self.hidden_layers, self.hidden_size + 1])\n",
    "        ##############\n",
    "        #####TODO Needs to go in reverse if we have multiple hidden\n",
    "        for h_layer_i in range(self.hidden_layers):\n",
    "            # If it is the last hidden layer, then we look at the activations and deltas\n",
    "            # of the output layer, not the next hidden layer\n",
    "            print(\"LAYER: %i\" % h_layer_i)\n",
    "            if h_layer_i == self.hidden_layers - 1:\n",
    "                # Loop over each in hidden activation, +1 for bias\n",
    "                for h_dim_j in range(self.hidden_size + 1):\n",
    "                    for k, delta_out in enumerate(self.deltas_out):\n",
    "                        self.Deltas_hidden[h_layer_i][h_dim_j] += delta_out * self._derivative_activation(self.activations_out[k]) * self.weights_out[h_dim_j][k]\n",
    "            else:\n",
    "                # Do the same to find the hidden deltas\n",
    "                for h_dim_j in range(self.hidden_size + 1):\n",
    "                    for k, delta_h in enumerate(self.Deltas_hidden[h_layer_i + 1]):\n",
    "                        self.Deltas_hidden[h_layer_i][h_dim_j] += delta_h * self._derivative_activation(self.activations_hidden[h_layer_i + 1][k]) * self.weights_hidden[h_layer_i][h_dim_j][k]\n",
    "                        \n",
    "        \"\"\"\n",
    "        Now just do the same for L'(input layer)\n",
    "        \"\"\"\n",
    "        self.deltas_in = np.zeros([self.input_size+1])\n",
    "        #### Need deltas_hidden[0].dot(der_act_hidden[0] * weights_in)\n",
    "        # Loop over each in input activation, +1 for bias\n",
    "        #for dim_i in range(self.input_size + 1):\n",
    "        #    for k, delta_h in enumerate(self.Deltas_hidden[0]):\n",
    "        #        self.deltas_in[dim_i] += delta_h * self._derivative_activation(self.activations_hidden[0][k]) * self.weights_in[dim_i][k]\n",
    "        \n",
    "        self.update_weights()\n",
    "        return loss\n",
    "        \n",
    "    def update_weights(self):\n",
    "        print(\"UPDATING WEIGHTS\")\n",
    "        # Now we can use the deltas to adjust each weight by L'(w_i_j)\n",
    "        # These weights are the edges shared between last hidden layer, and output layer\n",
    "        # Rows of weights_out correponds with length of last hidden layer\n",
    "        for i in range(len(self.weights_out)):\n",
    "            # Cols of weights_out correspinds with length of output layer\n",
    "            for j in range(len(self.weights_out[i])):\n",
    "                self.weights_out[i][j] -= self._activate(self.activations_hidden[-1][i])\\\n",
    "                * self._derivative_activation(self.activations_out[j])\\\n",
    "                * self.deltas_out[j]\\\n",
    "                * self.learning_rate\n",
    "                    \n",
    "        # Loop over each hidden layer\n",
    "        for w_i in reversed(range(len(self.weights_hidden))):\n",
    "            # Rows (i) in the weights for this layer will correspond to the size of the layer BEFORE (hidden or input)\n",
    "            for i in range(len(self.weights_hidden[w_i])):\n",
    "                # Cols (j) in these weights will correspond to the size of hidden layer w_i\n",
    "                for j in range(len(self.weights_hidden[w_i][i])):\n",
    "                    # We do not want to update the dummy 0 weight going TO the extra bias dimension\n",
    "                    if j == len(self.weights_hidden[w_i][i]) - 1:\n",
    "                        continue\n",
    "                    else:\n",
    "                        # + 1 for layer before, because we are looping in reverse\n",
    "                        self.weights_hidden[w_i][i][j] -= self._activate(self.activations_hidden[w_i + 1][i])\\\n",
    "                        * self._derivative_activation(self.activations_hidden[w_i][j])\\\n",
    "                        * self.Deltas_hidden[w_i][j]\\\n",
    "                        * self.learning_rate\n",
    "                        \n",
    "            #self.weights_hidden[w_i] -= self.activations_hidden[w_i].T.dot(self.Deltas_hidden[w_i]) * self.learning_rate\n",
    "        # Rows (i) of weights_in corresponds to size of the input layer\n",
    "        for i in range(len(self.weights_in)):\n",
    "            # Cols (j) corresponds to size of the first hidden layer (layer above input layer)\n",
    "            for j in range(len(self.weights_in[i])):\n",
    "                # We do not want to update the dummy 0 weight going TO the extra bias dimension\n",
    "                if j == len(self.weights_in[i]) - 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    \"\"\"\n",
    "                    print(\"act, deriv_act, delta, weight update val\")\n",
    "                    print(self.activations_in[i])\n",
    "                    print(self._derivative_activation(self.activations_hidden[0][j]))\n",
    "                    print(self.Deltas_hidden[0][j])\n",
    "                    print(self.activations_in[i]\\\n",
    "                    * self._derivative_activation(self.activations_hidden[0][j])\\\n",
    "                    * self.Deltas_hidden[0][j]\\\n",
    "                    * self.learning_rate)\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    self.weights_in[i][j] -= self.activations_in[i]\\\n",
    "                    * self._derivative_activation(self.activations_hidden[0][j])\\\n",
    "                    * self.Deltas_hidden[0][j]\\\n",
    "                    * self.learning_rate\n",
    "    \n",
    "    def train(self, inputs, targets, epochs=50, lr=.01):\n",
    "        self.learning_rate = lr\n",
    "        one_hot_targets = self._targets_to_one_hots(targets)\n",
    "        for e in range(epochs):\n",
    "            print(\"EPOCH %i\" % e)\n",
    "            \"\"\"\n",
    "            SGD - randomize the order of the training samples, and \n",
    "            \"\"\"\n",
    "            in_out = list(zip(inputs, one_hot_targets))\n",
    "            random.shuffle(in_out)\n",
    "            # For tracking average loss over SGD, just for logging\n",
    "            losses = []\n",
    "            \n",
    "            for inp, target in in_out:\n",
    "                if inp == [-1, -1]:\n",
    "                    print(\"TARGET: \")\n",
    "                    print(target)\n",
    "                    self.forward(inp)\n",
    "                    losses.append(self.backward(target))\n",
    "\n",
    "            print(\"LOSS: %2f\" % (sum(losses)/len(losses)))\n",
    "            \n",
    "\"\"\"\n",
    "Note that the 4th param, the size of the output layer, should be the\n",
    "number of classes\n",
    "\"\"\"\n",
    "#MLP = NN(2, 1, 2, 2)\n",
    "#xor_inputs = [[1, 1], [-1, 1], [-1, -1], [1, -1]]\n",
    "#xor_labels = ([1, 0, 1, 0])\n",
    "#MLP.train(xor_inputs, xor_labels)\n",
    "\"\"\"\n",
    "class Node(Object):\n",
    "    def __init__(self):\n",
    "        \n",
    "    def \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.12647828  1.54936555  0.        ]\n",
      " [ 0.96274059  0.01862787  0.        ]\n",
      " [-1.06116466  0.83846283  0.        ]]\n",
      "[]\n",
      "[[ 0.66192006 -1.57011581]\n",
      " [ 0.48394253  1.46526895]\n",
      " [ 0.70894469 -0.73593098]]\n",
      "SOFTMAX_LAYER\n",
      "[0.72509090015875832, 0.27490909984124173]\n",
      "LOSS PER INSTANCE: 1.291315\n",
      "DERIVATIVE LOSS: -3.637566\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.21830473  0.24685924]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.291315\n",
      "EPOCH 1\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.12843979  1.54129241  0.        ]\n",
      " [ 0.9647021   0.01055472  0.        ]\n",
      " [-1.06312618  0.84653598  0.        ]]\n",
      "[]\n",
      "[[ 0.66354728 -1.56803507]\n",
      " [ 0.48901781  1.47175878]\n",
      " [ 0.72035065 -0.72134605]]\n",
      "SOFTMAX_LAYER\n",
      "[0.72323297514351326, 0.27676702485648663]\n",
      "LOSS PER INSTANCE: 1.284579\n",
      "DERIVATIVE LOSS: -3.613147\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.21733528  0.24743178]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.284579\n",
      "EPOCH 2\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.13039479  1.53314537  0.        ]\n",
      " [ 0.9666571   0.00240769  0.        ]\n",
      " [-1.06508118  0.85468301  0.        ]]\n",
      "[]\n",
      "[[ 0.66514082 -1.56596962]\n",
      " [ 0.49409635  1.47834125]\n",
      " [ 0.73157964 -0.70679175]]\n",
      "SOFTMAX_LAYER\n",
      "[0.72131993243514636, 0.27868006756485375]\n",
      "LOSS PER INSTANCE: 1.277691\n",
      "DERIVATIVE LOSS: -3.588344\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.21635806  0.24795243]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.277691\n",
      "EPOCH 3\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.13234183  1.52492937  0.        ]\n",
      " [ 0.96860414 -0.00580831  0.        ]\n",
      " [-1.06702822  0.86289901  0.        ]]\n",
      "[]\n",
      "[[ 0.666701   -1.56392051]\n",
      " [ 0.49917688  1.48501392]\n",
      " [ 0.74263149 -0.69227647]]\n",
      "SOFTMAX_LAYER\n",
      "[0.71935295538605459, 0.28064704461394541]\n",
      "LOSS PER INSTANCE: 1.270657\n",
      "DERIVATIVE LOSS: -3.563194\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.21537342  0.24841891]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.270657\n",
      "EPOCH 4\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.13427947  1.51664964  0.        ]\n",
      " [ 0.97054178 -0.01408805  0.        ]\n",
      " [-1.06896586  0.87117875  0.        ]]\n",
      "[]\n",
      "[[ 0.66822815 -1.56188878]\n",
      " [ 0.50425812  1.49177405]\n",
      " [ 0.75350622 -0.67780863]]\n",
      "SOFTMAX_LAYER\n",
      "[0.71733338208709718, 0.28266661791290293]\n",
      "LOSS PER INSTANCE: 1.263487\n",
      "DERIVATIVE LOSS: -3.537736\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.21438171  0.24882902]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.263487\n",
      "EPOCH 5\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.13620628  1.50831171  0.        ]\n",
      " [ 0.97246859 -0.02242598  0.        ]\n",
      " [-1.07089266  0.87951668  0.        ]]\n",
      "[]\n",
      "[[ 0.66972264 -1.55987542]\n",
      " [ 0.50933877  1.49861861]\n",
      " [ 0.76420405 -0.6633967 ]]\n",
      "SOFTMAX_LAYER\n",
      "[0.71526270739427211, 0.28473729260572789]\n",
      "LOSS PER INSTANCE: 1.256188\n",
      "DERIVATIVE LOSS: -3.512009\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.21338328  0.24918067]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.256188\n",
      "EPOCH 6\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.13812084  1.49992139  0.        ]\n",
      " [ 0.97438315 -0.03081629  0.        ]\n",
      " [-1.07280723  0.88790699  0.        ]]\n",
      "[]\n",
      "[[ 0.67118487 -1.55788144]\n",
      " [ 0.51441751  1.50554431]\n",
      " [ 0.77472539 -0.64904911]]\n",
      "SOFTMAX_LAYER\n",
      "[0.71314258371600325, 0.2868574162839968]\n",
      "LOSS PER INSTANCE: 1.248770\n",
      "DERIVATIVE LOSS: -3.486052\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.21237852  0.24947191]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.248770\n",
      "EPOCH 7\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.14002178  1.49148478  0.        ]\n",
      " [ 0.97628409 -0.0392529   0.        ]\n",
      " [-1.07470817  0.8963436   0.        ]]\n",
      "[]\n",
      "[[ 0.67261525 -1.55590777]\n",
      " [ 0.51949298  1.51254755]\n",
      " [ 0.78507085 -0.63477425]]\n",
      "SOFTMAX_LAYER\n",
      "[0.71097482042719085, 0.2890251795728091]\n",
      "LOSS PER INSTANCE: 1.241241\n",
      "DERIVATIVE LOSS: -3.459906\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.21136782  0.24970097]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.241241\n",
      "EPOCH 8\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.14190774  1.48300819  0.        ]\n",
      " [ 0.97817005 -0.0477295   0.        ]\n",
      " [-1.07659413  0.90482019  0.        ]]\n",
      "[]\n",
      "[[ 0.67401423 -1.55395535]\n",
      " [ 0.52456384  1.51962447]\n",
      " [ 0.79524122 -0.62058042]]\n",
      "SOFTMAX_LAYER\n",
      "[0.70876138179270964, 0.29123861820729036]\n",
      "LOSS PER INSTANCE: 1.233612\n",
      "DERIVATIVE LOSS: -3.433611\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.21035159  0.24986624]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.233612\n",
      "EPOCH 9\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.1437774   1.47449815  0.        ]\n",
      " [ 0.98003971 -0.05623953  0.        ]\n",
      " [-1.07846379  0.91333023  0.        ]]\n",
      "[]\n",
      "[[ 0.67538228 -1.55202505]\n",
      " [ 0.5296287   1.52677094]\n",
      " [ 0.80523747 -0.60647582]]\n",
      "SOFTMAX_LAYER\n",
      "[0.70650438330503984, 0.2934956166949601]\n",
      "LOSS PER INSTANCE: 1.225893\n",
      "DERIVATIVE LOSS: -3.407206\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.20933025  0.24996632]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.225893\n",
      "EPOCH 10\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.14562949  1.46596139  0.        ]\n",
      " [ 0.9818918  -0.0647763   0.        ]\n",
      " [-1.08031588  0.92186699  0.        ]]\n",
      "[]\n",
      "[[ 0.67671988 -1.55011771]\n",
      " [ 0.5346862   1.53398258]\n",
      " [ 0.81506077 -0.59246846]]\n",
      "SOFTMAX_LAYER\n",
      "[0.70420608636803028, 0.29579391363196972]\n",
      "LOSS PER INSTANCE: 1.218092\n",
      "DERIVATIVE LOSS: -3.380732\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.20830426  0.25      ]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.218092\n",
      "EPOCH 11\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.14746278  1.45740477  0.        ]\n",
      " [ 0.98372509 -0.07333291  0.        ]\n",
      " [-1.08214917  0.93042361  0.        ]]\n",
      "[]\n",
      "[[ 0.67802755 -1.54823414]\n",
      " [ 0.53973493  1.54125478]\n",
      " [ 0.82471242 -0.5785662 ]]\n",
      "SOFTMAX_LAYER\n",
      "[0.70186889129086172, 0.29813110870913823]\n",
      "LOSS PER INSTANCE: 1.210222\n",
      "DERIVATIVE LOSS: -3.354229\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.20727406  0.24996634]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.210222\n",
      "EPOCH 12\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.14927609  1.44883528  0.        ]\n",
      " [ 0.9855384  -0.0819024   0.        ]\n",
      " [-1.08396248  0.9389931   0.        ]]\n",
      "[]\n",
      "[[ 0.67930582 -1.54637507]\n",
      " [ 0.54477352  1.54858273]\n",
      " [ 0.83419392 -0.56477664]]\n",
      "SOFTMAX_LAYER\n",
      "[0.69949532859234942, 0.30050467140765064]\n",
      "LOSS PER INSTANCE: 1.202292\n",
      "DERIVATIVE LOSS: -3.327735\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.20624013  0.24986461]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.202292\n",
      "EPOCH 13\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.15106831  1.44026     0.        ]\n",
      " [ 0.98733063 -0.09047769  0.        ]\n",
      " [-1.0857547   0.94756839  0.        ]]\n",
      "[]\n",
      "[[ 0.68055523 -1.5445412 ]\n",
      " [ 0.54980059  1.5559614 ]\n",
      " [ 0.84350693 -0.55110713]]\n",
      "SOFTMAX_LAYER\n",
      "[0.69708804865466933, 0.30291195134533061]\n",
      "LOSS PER INSTANCE: 1.194313\n",
      "DERIVATIVE LOSS: -3.301289\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.20520298  0.24969438]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.194313\n",
      "EPOCH 14\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.1528384   1.43168601  0.        ]\n",
      " [ 0.98910071 -0.09905167  0.        ]\n",
      " [-1.08752479  0.95614237  0.        ]]\n",
      "[]\n",
      "[[ 0.68177634 -1.54273318]\n",
      " [ 0.55481477  1.56338561]\n",
      " [ 0.85265323 -0.53756473]]\n",
      "SOFTMAX_LAYER\n",
      "[0.69464980980610547, 0.30535019019389459]\n",
      "LOSS PER INSTANCE: 1.186296\n",
      "DERIVATIVE LOSS: -3.274928\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.2041631   0.24945545]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.186296\n",
      "EPOCH 15\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.15458536  1.42312045  0.        ]\n",
      " [ 0.99084767 -0.10761724  0.        ]\n",
      " [-1.08927175  0.96470794  0.        ]]\n",
      "[]\n",
      "[[ 0.68296972 -1.54095159]\n",
      " [ 0.55981471  1.57085003]\n",
      " [ 0.86163478 -0.52415616]]\n",
      "SOFTMAX_LAYER\n",
      "[0.69218346495293948, 0.30781653504706058]\n",
      "LOSS PER INSTANCE: 1.178251\n",
      "DERIVATIVE LOSS: -3.248688\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.20312103  0.24914791]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.178251\n",
      "EPOCH 16\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.15630828  1.41457037  0.        ]\n",
      " [ 0.9925706  -0.11616731  0.        ]\n",
      " [-1.09099467  0.97325801  0.        ]]\n",
      "[]\n",
      "[[ 0.68413594 -1.53919694]\n",
      " [ 0.56479907  1.57834922]\n",
      " [ 0.87045364 -0.5108878 ]]\n",
      "SOFTMAX_LAYER\n",
      "[0.68969194691952307, 0.31030805308047699]\n",
      "LOSS PER INSTANCE: 1.170190\n",
      "DERIVATIVE LOSS: -3.222604\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.20207729  0.24877212]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.170190\n",
      "EPOCH 17\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.15800633  1.40604278  0.        ]\n",
      " [ 0.99426864 -0.12469491  0.        ]\n",
      " [-1.09269272  0.98178561  0.        ]]\n",
      "[]\n",
      "[[ 0.68527561 -1.53746972]\n",
      " [ 0.56976655  1.58587765]\n",
      " [ 0.87911202 -0.49776564]]\n",
      "SOFTMAX_LAYER\n",
      "[0.68717825269120159, 0.31282174730879836]\n",
      "LOSS PER INSTANCE: 1.162122\n",
      "DERIVATIVE LOSS: -3.196709\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.20103242  0.24832873]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.162122\n",
      "EPOCH 18\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.15967874  1.39754454  0.        ]\n",
      " [ 0.99594105 -0.13319315  0.        ]\n",
      " [-1.09436513  0.99028384  0.        ]]\n",
      "[]\n",
      "[[ 0.68638933 -1.53577033]\n",
      " [ 0.57471587  1.59342974]\n",
      " [ 0.88761223 -0.48479529]]\n",
      "SOFTMAX_LAYER\n",
      "[0.68464542678554541, 0.31535457321445465]\n",
      "LOSS PER INSTANCE: 1.154058\n",
      "DERIVATIVE LOSS: -3.171034\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.19998699  0.24781866]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.154058\n",
      "EPOCH 19\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.16132483  1.38908239  0.        ]\n",
      " [ 0.99758714 -0.1416553   0.        ]\n",
      " [-1.09601122  0.998746    0.        ]]\n",
      "[]\n",
      "[[ 0.68747768 -1.5340991 ]\n",
      " [ 0.57964578  1.60099987]\n",
      " [ 0.89595671 -0.47198192]]\n",
      "SOFTMAX_LAYER\n",
      "[0.6820965440018727, 0.31790345599812742]\n",
      "LOSS PER INSTANCE: 1.146008\n",
      "DERIVATIVE LOSS: -3.145609\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.19894154  0.24724306]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.146008\n",
      "EPOCH 20\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.16294399  1.38066284  0.        ]\n",
      " [ 0.9992063  -0.15007485  0.        ]\n",
      " [-1.09763038  1.00716554  0.        ]]\n",
      "[]\n",
      "[[ 0.68854129 -1.53245632]\n",
      " [ 0.58455507  1.60858244]\n",
      " [ 0.90414796 -0.45933025]]\n",
      "SOFTMAX_LAYER\n",
      "[0.67953469181623194, 0.32046530818376812]\n",
      "LOSS PER INSTANCE: 1.137981\n",
      "DERIVATIVE LOSS: -3.120463\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.19789663  0.24660339]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.137981\n",
      "EPOCH 21\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.16453569  1.37229221  0.        ]\n",
      " [ 1.000798   -0.15844547  0.        ]\n",
      " [-1.09922208  1.01553617  0.        ]]\n",
      "[]\n",
      "[[ 0.68958076 -1.53084221]\n",
      " [ 0.58944259  1.61617188]\n",
      " [ 0.9121886  -0.44684459]]\n",
      "SOFTMAX_LAYER\n",
      "[0.67696295269806206, 0.32303704730193805]\n",
      "LOSS PER INSTANCE: 1.129988\n",
      "DERIVATIVE LOSS: -3.095620\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.19685283  0.2459013 ]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.129988\n",
      "EPOCH 22\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.1660995   1.36397655  0.        ]\n",
      " [ 1.00236181 -0.16676114  0.        ]\n",
      " [-1.10078589  1.02385183  0.        ]]\n",
      "[]\n",
      "[[ 0.6905967  -1.52925693]\n",
      " [ 0.59430722  1.62376269]\n",
      " [ 0.9200813  -0.43452875]]\n",
      "SOFTMAX_LAYER\n",
      "[0.67438438662532452, 0.32561561337467559]\n",
      "LOSS PER INSTANCE: 1.122038\n",
      "DERIVATIVE LOSS: -3.071106\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.19581068  0.2451387 ]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.122038\n",
      "EPOCH 23\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.16763503  1.35572162  0.        ]\n",
      " [ 1.00389735 -0.17501606  0.        ]\n",
      " [-1.10232142  1.03210676  0.        ]]\n",
      "[]\n",
      "[[ 0.69158973 -1.52770057]\n",
      " [ 0.59914789  1.63134944]\n",
      " [ 0.92782881 -0.4223861 ]]\n",
      "SOFTMAX_LAYER\n",
      "[0.67180201406706352, 0.32819798593293653]\n",
      "LOSS PER INSTANCE: 1.114138\n",
      "DERIVATIVE LOSS: -3.046941\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.19477074  0.24431769]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.114138\n",
      "EPOCH 24\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.16914202  1.34753289  0.        ]\n",
      " [ 1.00540433 -0.18320479  0.        ]\n",
      " [-1.10382841  1.04029549  0.        ]]\n",
      "[]\n",
      "[[ 0.69256044 -1.52617317]\n",
      " [ 0.60396358  1.63892685]\n",
      " [ 0.93543394 -0.41041956]]\n",
      "SOFTMAX_LAYER\n",
      "[0.66921879968656051, 0.33078120031343955]\n",
      "LOSS PER INSTANCE: 1.106298\n",
      "DERIVATIVE LOSS: -3.023146\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.19373353  0.24344055]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.106298\n",
      "EPOCH 25\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.17062023  1.33941549  0.        ]\n",
      " [ 1.00688254 -0.19132219  0.        ]\n",
      " [-1.10530662  1.04841289  0.        ]]\n",
      "[]\n",
      "[[ 0.69350944 -1.52467472]\n",
      " [ 0.60875333  1.64648978]\n",
      " [ 0.94289951 -0.39863158]]\n",
      "SOFTMAX_LAYER\n",
      "[0.66663763699537448, 0.33336236300462541]\n",
      "LOSS PER INSTANCE: 1.098525\n",
      "DERIVATIVE LOSS: -2.999739\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.1926996   0.24250973]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.098525\n",
      "EPOCH 26\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.17206953  1.33137422  0.        ]\n",
      " [ 1.00833184 -0.19936347  0.        ]\n",
      " [-1.10675592  1.05645417  0.        ]]\n",
      "[]\n",
      "[[ 0.69443733 -1.52320514]\n",
      " [ 0.61351625  1.65403322]\n",
      " [ 0.95022843 -0.38702414]]\n",
      "SOFTMAX_LAYER\n",
      "[0.664061334159742, 0.33593866584025805]\n",
      "LOSS PER INSTANCE: 1.090827\n",
      "DERIVATIVE LOSS: -2.976734\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.19166944  0.24152783]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.090827\n",
      "EPOCH 27\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.17348985  1.3234135   0.        ]\n",
      " [ 1.00975217 -0.20732418  0.        ]\n",
      " [-1.10817624  1.06441488  0.        ]]\n",
      "[]\n",
      "[[ 0.69534469 -1.52176431]\n",
      " [ 0.61825148  1.6615524 ]\n",
      " [ 0.95742359 -0.3755988 ]]\n",
      "SOFTMAX_LAYER\n",
      "[0.66149260112743546, 0.33850739887256442]\n",
      "LOSS PER INSTANCE: 1.083209\n",
      "DERIVATIVE LOSS: -2.954145\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.19064357  0.24049754]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.083209\n",
      "EPOCH 28\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.17488119  1.31553742  0.        ]\n",
      " [ 1.0111435  -0.21520027  0.        ]\n",
      " [-1.10956758  1.07229096  0.        ]]\n",
      "[]\n",
      "[[ 0.69623212 -1.52035207]\n",
      " [ 0.62295825  1.6690427 ]\n",
      " [ 0.96448792 -0.36435669]]\n",
      "SOFTMAX_LAYER\n",
      "[0.65893403820676466, 0.34106596179323523]\n",
      "LOSS PER INSTANCE: 1.075679\n",
      "DERIVATIVE LOSS: -2.931984\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.18962244  0.23942167]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.075679\n",
      "EPOCH 29\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.17624361  1.30774968  0.        ]\n",
      " [ 1.01250592 -0.22298801  0.        ]\n",
      " [-1.11093     1.08007871  0.        ]]\n",
      "[]\n",
      "[[ 0.69710018 -1.51896819]\n",
      " [ 0.62763583  1.67649977]\n",
      " [ 0.97142434 -0.35329852]]\n",
      "SOFTMAX_LAYER\n",
      "[0.65638812619148701, 0.34361187380851299]\n",
      "LOSS PER INSTANCE: 1.068243\n",
      "DERIVATIVE LOSS: -2.910260\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.18860652  0.23830308]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.068243\n",
      "EPOCH 30\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.17757722  1.30005361  0.        ]\n",
      " [ 1.01383953 -0.23068407  0.        ]\n",
      " [-1.11226361  1.08777477  0.        ]]\n",
      "[]\n",
      "[[ 0.69794944 -1.51761241]\n",
      " [ 0.63228354  1.68391946]\n",
      " [ 0.9782358  -0.34242461]]\n",
      "SOFTMAX_LAYER\n",
      "[0.65385721808748232, 0.34614278191251779]\n",
      "LOSS PER INSTANCE: 1.060904\n",
      "DERIVATIVE LOSS: -2.888981\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.18759625  0.23714468]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.060904\n",
      "EPOCH 31\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.1788822   1.29245219  0.        ]\n",
      " [ 1.01514451 -0.2382855   0.        ]\n",
      " [-1.11356859  1.0953762   0.        ]]\n",
      "[]\n",
      "[[ 0.69878046 -1.51628444]\n",
      " [ 0.6369008   1.69129786]\n",
      " [ 0.98492521 -0.33173489]]\n",
      "SOFTMAX_LAYER\n",
      "[0.65134353246049004, 0.34865646753950991]\n",
      "LOSS PER INSTANCE: 1.053668\n",
      "DERIVATIVE LOSS: -2.868153\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.18659203  0.23594939]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.053668\n",
      "EPOCH 32\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.18015878  1.28494802  0.        ]\n",
      " [ 1.01642109 -0.24578967  0.        ]\n",
      " [-1.11484517  1.10288037  0.        ]]\n",
      "[]\n",
      "[[ 0.69959378 -1.51498394]\n",
      " [ 0.64148704  1.69863132]\n",
      " [ 0.99149548 -0.32122896]]\n",
      "SOFTMAX_LAYER\n",
      "[0.64884914839017915, 0.35115085160982079]\n",
      "LOSS PER INSTANCE: 1.046539\n",
      "DERIVATIVE LOSS: -2.847779\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.18559424  0.23472016]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.046539\n",
      "EPOCH 33\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.18140724  1.27754335  0.        ]\n",
      " [ 1.01766955 -0.25319434  0.        ]\n",
      " [-1.11609363  1.11028503  0.        ]]\n",
      "[]\n",
      "[[ 0.70038993 -1.51371054]\n",
      " [ 0.64604179  1.70591643]\n",
      " [ 0.99794949 -0.31090605]]\n",
      "SOFTMAX_LAYER\n",
      "[0.64637600198525857, 0.35362399801474148]\n",
      "LOSS PER INSTANCE: 1.039521\n",
      "DERIVATIVE LOSS: -2.827862\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.18460325  0.23345989]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.039521\n",
      "EPOCH 34\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.18262789  1.2702401   0.        ]\n",
      " [ 1.0188902  -0.26049759  0.        ]\n",
      " [-1.11731427  1.11758829  0.        ]]\n",
      "[]\n",
      "[[ 0.70116942 -1.51246384]\n",
      " [ 0.65056461  1.71315005]\n",
      " [ 1.00429011 -0.30076513]]\n",
      "SOFTMAX_LAYER\n",
      "[0.64392588438791465, 0.35607411561208535]\n",
      "LOSS PER INSTANCE: 1.032616\n",
      "DERIVATIVE LOSS: -2.808404\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.18361939  0.23217145]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.032616\n",
      "EPOCH 35\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.18382108  1.26303984  0.        ]\n",
      " [ 1.02008339 -0.26769784  0.        ]\n",
      " [-1.11850747  1.12478854  0.        ]]\n",
      "[]\n",
      "[[ 0.70193278 -1.51124343]\n",
      " [ 0.65505513  1.72032926]\n",
      " [ 1.01052016 -0.29080484]]\n",
      "SOFTMAX_LAYER\n",
      "[0.64150044117400473, 0.35849955882599532]\n",
      "LOSS PER INSTANCE: 1.025828\n",
      "DERIVATIVE LOSS: -2.789404\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.18264296  0.23085768]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.025828\n",
      "EPOCH 36\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.18498722  1.25594385  0.        ]\n",
      " [ 1.02124953 -0.27479384  0.        ]\n",
      " [-1.11967361  1.13188454  0.        ]]\n",
      "[]\n",
      "[[ 0.70268049 -1.51004885]\n",
      " [ 0.65951303  1.72745144]\n",
      " [ 1.01664242 -0.28102359]]\n",
      "SOFTMAX_LAYER\n",
      "[0.63910117303831837, 0.36089882696168157]\n",
      "LOSS PER INSTANCE: 1.019158\n",
      "DERIVATIVE LOSS: -2.770860\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.18167426  0.22952133]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.019158\n",
      "EPOCH 37\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.18612671  1.24895307  0.        ]\n",
      " [ 1.02238902 -0.28178461  0.        ]\n",
      " [-1.1208131   1.13887531  0.        ]]\n",
      "[]\n",
      "[[ 0.70341303 -1.50887964]\n",
      " [ 0.66393802  1.73451416]\n",
      " [ 1.02265964 -0.27141952]]\n",
      "SOFTMAX_LAYER\n",
      "[0.63672943764180756, 0.36327056235819249]\n",
      "LOSS PER INSTANCE: 1.012607\n",
      "DERIVATIVE LOSS: -2.752769\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.18071352  0.22816507]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.012607\n",
      "EPOCH 38\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.18724002  1.2420682   0.        ]\n",
      " [ 1.02350233 -0.28866949  0.        ]\n",
      " [-1.1219264   1.14576018  0.        ]]\n",
      "[]\n",
      "[[ 0.70413087 -1.50773532]\n",
      " [ 0.66832991  1.74151529]\n",
      " [ 1.02857452 -0.26199058]]\n",
      "SOFTMAX_LAYER\n",
      "[0.6343864524897519, 0.36561354751024816]\n",
      "LOSS PER INSTANCE: 1.006178\n",
      "DERIVATIVE LOSS: -2.735128\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.17976099  0.22679149]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.006178\n",
      "EPOCH 39\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.1883276   1.23528964  0.        ]\n",
      " [ 1.02458991 -0.29544804  0.        ]\n",
      " [-1.12301399  1.15253874  0.        ]]\n",
      "[]\n",
      "[[ 0.70483447 -1.5066154 ]\n",
      " [ 0.6726885   1.74845288]\n",
      " [ 1.0343897  -0.25273452]]\n",
      "SOFTMAX_LAYER\n",
      "[0.63207329870601392, 0.36792670129398602]\n",
      "LOSS PER INSTANCE: 0.999872\n",
      "DERIVATIVE LOSS: -2.717933\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.17881687  0.22540308]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 0.999872\n",
      "EPOCH 40\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.18938995  1.22861757  0.        ]\n",
      " [ 1.02565227 -0.30212012  0.        ]\n",
      " [-1.12407634  1.15921082  0.        ]]\n",
      "[]\n",
      "[[ 0.70552427 -1.50551937]\n",
      " [ 0.67701367  1.75532524]\n",
      " [ 1.04010778 -0.24364893]]\n",
      "SOFTMAX_LAYER\n",
      "[0.62979092556836813, 0.37020907443163181]\n",
      "LOSS PER INSTANCE: 0.993687\n",
      "DERIVATIVE LOSS: -2.701176\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.17788133  0.22400222]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 0.993687\n",
      "EPOCH 41\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.19042759  1.22205191  0.        ]\n",
      " [ 1.0266899  -0.30868578  0.        ]\n",
      " [-1.12511397  1.16577648  0.        ]]\n",
      "[]\n",
      "[[ 0.70620069 -1.50444672]\n",
      " [ 0.68130534  1.7621309 ]\n",
      " [ 1.0457313  -0.23473125]]\n",
      "SOFTMAX_LAYER\n",
      "[0.62754015567285126, 0.37245984432714874]\n",
      "LOSS PER INSTANCE: 0.987626\n",
      "DERIVATIVE LOSS: -2.684853\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.17695453  0.22259118]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 0.987626\n",
      "EPOCH 42\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.19144101  1.21559239  0.        ]\n",
      " [ 1.02770332 -0.3151453   0.        ]\n",
      " [-1.1261274   1.172236    0.        ]]\n",
      "[]\n",
      "[[ 0.70686414 -1.50339692]\n",
      " [ 0.68556345  1.76886857]\n",
      " [ 1.05126275 -0.22597877]]\n",
      "SOFTMAX_LAYER\n",
      "[0.62532169060060039, 0.37467830939939967]\n",
      "LOSS PER INSTANCE: 0.981687\n",
      "DERIVATIVE LOSS: -2.668956\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.17603661  0.22117211]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 0.981687\n",
      "EPOCH 43\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.19243076  1.20923853  0.        ]\n",
      " [ 1.02869307 -0.32149916  0.        ]\n",
      " [-1.12711715  1.17858985  0.        ]]\n",
      "[]\n",
      "[[ 0.70751504 -1.50236945]\n",
      " [ 0.689788    1.77553717]\n",
      " [ 1.05670454 -0.21738869]]\n",
      "SOFTMAX_LAYER\n",
      "[0.62313611696817994, 0.37686388303182017]\n",
      "LOSS PER INSTANCE: 0.975871\n",
      "DERIVATIVE LOSS: -2.653478\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.17512767  0.21974707]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 0.975871\n",
      "EPOCH 44\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.19339738  1.20298969  0.        ]\n",
      " [ 1.02965969 -0.32774799  0.        ]\n",
      " [-1.12808376  1.18483869  0.        ]]\n",
      "[]\n",
      "[[ 0.70815376 -1.5013638 ]\n",
      " [ 0.69397901  1.78213583]\n",
      " [ 1.06205905 -0.20895813]]\n",
      "SOFTMAX_LAYER\n",
      "[0.62098391275141018, 0.37901608724858987]\n",
      "LOSS PER INSTANCE: 0.970177\n",
      "DERIVATIVE LOSS: -2.638410\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.17422782  0.21831795]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 0.970177\n",
      "EPOCH 45\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.19434139  1.19684506  0.        ]\n",
      " [ 1.0306037  -0.33389262  0.        ]\n",
      " [-1.12902778  1.19098332  0.        ]]\n",
      "[]\n",
      "[[ 0.70878069 -1.50037943]\n",
      " [ 0.69813654  1.78866381]\n",
      " [ 1.06732859 -0.20068413]]\n",
      "SOFTMAX_LAYER\n",
      "[0.61886545378272229, 0.38113454621727777]\n",
      "LOSS PER INSTANCE: 0.964603\n",
      "DERIVATIVE LOSS: -2.623745\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.17333712  0.21688658]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 0.964603\n",
      "EPOCH 46\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.19526335  1.19080369  0.        ]\n",
      " [ 1.03152566 -0.33993399  0.        ]\n",
      " [-1.12994974  1.19702469  0.        ]]\n",
      "[]\n",
      "[[ 0.70939617 -1.49941582]\n",
      " [ 0.70226067  1.79512058]\n",
      " [ 1.07251539 -0.19256365]]\n",
      "SOFTMAX_LAYER\n",
      "[0.61678102033263227, 0.38321897966736768]\n",
      "LOSS PER INSTANCE: 0.959149\n",
      "DERIVATIVE LOSS: -2.609474\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.17245563  0.21545462]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 0.959149\n",
      "EPOCH 47\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.19616379  1.1848645   0.        ]\n",
      " [ 1.0324261  -0.34587318  0.        ]\n",
      " [-1.13085018  1.20296388  0.        ]]\n",
      "[]\n",
      "[[ 0.71000057 -1.49847245]\n",
      " [ 0.70635153  1.80150573]\n",
      " [ 1.07762164 -0.18459363]]\n",
      "SOFTMAX_LAYER\n",
      "[0.61473080369669375, 0.38526919630330619]\n",
      "LOSS PER INSTANCE: 0.953813\n",
      "DERIVATIVE LOSS: -2.595588\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.17158339  0.21402366]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 0.953813\n",
      "EPOCH 48\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.19704327  1.1790263   0.        ]\n",
      " [ 1.03330558 -0.35171139  0.        ]\n",
      " [-1.13172966  1.20880209  0.        ]]\n",
      "[]\n",
      "[[ 0.71059423 -1.4975488 ]\n",
      " [ 0.71040925  1.80781901]\n",
      " [ 1.08264947 -0.17677098]]\n",
      "SOFTMAX_LAYER\n",
      "[0.61271491271993539, 0.38728508728006472]\n",
      "LOSS PER INSTANCE: 0.948594\n",
      "DERIVATIVE LOSS: -2.582077\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.17072041  0.21259514]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 0.948594\n",
      "EPOCH 49\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.19790231  1.17328778  0.        ]\n",
      " [ 1.03416463 -0.3574499   0.        ]\n",
      " [-1.1325887   1.2145406   0.        ]]\n",
      "[]\n",
      "[[ 0.71117745 -1.49664437]\n",
      " [ 0.71443399  1.8140603 ]\n",
      " [ 1.08760095 -0.16909258]]\n",
      "SOFTMAX_LAYER\n",
      "[0.61073338020107859, 0.3892666197989213]\n",
      "LOSS PER INSTANCE: 0.943491\n",
      "DERIVATIVE LOSS: -2.568933\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.16986671  0.21117042]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 0.943491\n"
     ]
    }
   ],
   "source": [
    "class SigmoidNN(NN):\n",
    "    def _activate(self, x):\n",
    "        \"\"\"\n",
    "        override RelU with sigmoid\n",
    "        \"\"\"\n",
    "        return self._sigmoid(x)\n",
    "    \n",
    "    def _derivative_activation(self, x):\n",
    "        \"\"\"\n",
    "        Override RelU' with Sigmoid'\n",
    "        \"\"\"\n",
    "        return self._derivative_sigmoid(x)\n",
    "    \n",
    "\"\"\"\n",
    "Run the same test with sigmoid\n",
    "\"\"\"\n",
    "MLP = SigmoidNN(2, 1, 2, 2)\n",
    "xor_inputs = [[1, 1], [-1, 1], [-1, -1], [1, -1]]\n",
    "xor_labels = ([1, 0, 1, 0])\n",
    "MLP.train(xor_inputs, xor_labels, epochs=50, lr=.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.09773946 -0.8236054   0.        ]\n",
      " [ 0.42621244 -0.66779998  0.        ]\n",
      " [-2.14975304 -0.93959653  0.        ]]\n",
      "[]\n",
      "[[-0.84511763  0.27598692]\n",
      " [ 0.50696887  1.09852717]\n",
      " [-0.88857492  0.63128769]]\n",
      "SOFTMAX_LAYER\n",
      "[0.75973241513799716, 0.24026758486200278]\n",
      "LOSS PER INSTANCE: 1.426002\n",
      "DERIVATIVE LOSS: -4.162026\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[-1.72504215  0.75044268]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.426002\n",
      "EPOCH 1\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-5.33653148 -1.37781277  0.        ]\n",
      " [-5.0080585  -1.22200734  0.        ]\n",
      " [ 3.28451791 -0.38538916  0.        ]]\n",
      "[]\n",
      "[[-1.94922632  0.06703447]\n",
      " [ 1.06639408  1.20439833]\n",
      " [-0.03964791  0.79194703]]\n",
      "SOFTMAX_LAYER\n",
      "[3.7306204634069575e-13, 0.99999999999962708]\n",
      "LOSS PER INSTANCE: 0.000000\n",
      "DERIVATIVE LOSS: -1.000000\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[-586.7913026   -18.11984055]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 0.000000\n",
      "EPOCH 2\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-11159599.13033338    129141.94603528         0.        ]\n",
      " [-11159598.80186039    129142.1018407          0.        ]\n",
      " [ 11159597.0783198    -129143.7092372          0.        ]]\n",
      "[]\n",
      "[[  3.09872137e+04   2.96166104e+01]\n",
      " [  3.02596626e+04   3.00573456e+01]\n",
      " [  2.36011258e+04   2.32967314e+01]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ -1.05204237e+24  -9.60171481e+17]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 3\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 4\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 5\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 6\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 7\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 8\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 9\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 10\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 11\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 12\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 13\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 14\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 15\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 16\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 17\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 18\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 19\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 20\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 21\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 22\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 23\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 24\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 25\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 26\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 27\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 28\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 29\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 30\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 31\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 32\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 33\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 34\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 35\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 36\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 37\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 38\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 39\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 40\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 41\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 42\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 43\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 44\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 45\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 46\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 47\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 48\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 49\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:61: RuntimeWarning: overflow encountered in exp\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:63: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "class TanHNN(NN):\n",
    "    \n",
    "    def _activate(self, x):\n",
    "        \"\"\"\n",
    "        Override with TanH\n",
    "        \"\"\"\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def _derivative_activation(self, x):\n",
    "        \"\"\"\n",
    "        Override with Derivative TanH (1 - tanH squared)\n",
    "        \"\"\"\n",
    "        return 1 - x*x\n",
    "    \n",
    "\"\"\"\n",
    "Run the same test with sigmoid\n",
    "\"\"\"\n",
    "MLP = TanHNN(2, 1, 2, 2)\n",
    "xor_inputs = [[1, 1], [-1, 1], [-1, -1], [1, -1]]\n",
    "xor_labels = ([1, 0, 1, 0])\n",
    "MLP.train(xor_inputs, xor_labels, epochs=50, lr=.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def neighborhood2int(n):\n",
    "    \"\"\"\n",
    "    For mapping neighborhoods into integer labels\n",
    "    \"\"\"\n",
    "    return 1 if n == \"Blmngtn\" else 0\n",
    "\n",
    "df = pd.DataFrame.from_csv(\"./data/housing_date_train_2_features.csv\")\n",
    "housing_df = df.loc[df['Neighborhood'].isin([\"BrDale\", \"Blmngtn\"])]\n",
    "\n",
    "# Grab X and its labels as a single matrix\n",
    "dataset = housing_df[[\"LotArea\", \"SalePrice\", \"Neighborhood\"]].as_matrix()\n",
    "\n",
    "# 'shuffle' the matrix to randomize the position of each sample in it\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "# Not sure why shuffling changed the type, but need to \n",
    "# change the type of each scalar in X for some numpy methods to work\n",
    "X = dataset[:, :2].astype('int64')\n",
    "# Encode the labels\n",
    "y = [neighborhood2int(s) for s in dataset[:, 2]]\n",
    "\n",
    "# Find the length of 80% of the dataset for training data\n",
    "train_length = int(X.shape[0] * 0.8)\n",
    "\n",
    "# split 80%/20% for train and test\n",
    "train_X = X[:train_length]\n",
    "test_X = X[train_length:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
