{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Node(Object):\\n    def __init__(self):\\n        \\n    def \\n'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class NN():\n",
    "    \"\"\"\n",
    "    Class for a neural network. Requires input/output sizes, number of hidden layers, and number of neurons\n",
    "    at each layer (we assume all hidden layers are of the same size)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, num_HL, hidden_size, output_size):\n",
    "        # Initialize by setting random \n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = num_HL\n",
    "        # NOTE we are assuming all hidden layers are the same size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Activations for each neuron\n",
    "        # + 1 for bias neuron\n",
    "        self.activations_in = np.ones(self.input_size + 1)\n",
    "        # Hidden can comprise multiple layers, so we have a matrix\n",
    "        # + 1 for bias neuron\n",
    "        self.activations_hidden = np.ones((self.hidden_layers, self.hidden_size + 1))\n",
    "        self.activations_out = np.ones(self.output_size)\n",
    "        # Weights of all the edges, randomized for good results\n",
    "        # PLUS ONE FOR BIASES\n",
    "        self.weights_in = np.random.randn(self.input_size + 1, self.hidden_size)\n",
    "        self.weights_in = np.column_stack((self.weights_in, np.zeros(self.input_size + 1)))\n",
    "        # We will only have hidden weights if there are multiple hidden layers\n",
    "        if self.hidden_layers > 1:\n",
    "            self.weights_hidden = np.random.randn(self.hidden_layers - 1, self.hidden_size + 1, self.hidden_size + 1)\n",
    "            # Set the weights corresponding with next layers biases to 0\n",
    "            for weights in self.weights_hidden:\n",
    "                weights[-1] = 0.0\n",
    "        else:\n",
    "            self.weights_hidden = []\n",
    "        # No plus one for output, as it should not have a bias parameter\n",
    "        self.weights_out = np.random.randn(self.hidden_size + 1, self.output_size)\n",
    "        # To be valued when train() is called\n",
    "        self.learning_rate = 0.0\n",
    "\n",
    "        # Instantiate deltas for holding gradients\n",
    "        self.deltas_in = []\n",
    "        self.Deltas_hidden = []\n",
    "        self.deltas_out = []\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Sigmoid function for calculating a distribution over 2 classes\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def _derivative_sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Derivative of the sigmoid function where x = the output of the sigmoid\n",
    "        \n",
    "        This can be used in backpropogation, wherein we would have \n",
    "        already computed the sigmoid in the forward pass, and we can draw upon its cached value\n",
    "        \"\"\"\n",
    "        return self._sigmoid(x) * (1.0 - self._sigmoid(x))\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        exponentials = [np.exp(p) for p in x]\n",
    "        denominator = sum(exponentials)\n",
    "        return [p / denominator for p in exponentials]\n",
    "    \n",
    "    def _relu(self, x):\n",
    "        \"\"\"\n",
    "        relu function used for activation\n",
    "        \"\"\"\n",
    "        return max(x, 0.0)\n",
    "    \n",
    "    def _derivative_relu(self, x):\n",
    "        \"\"\"\n",
    "        Derivative of the relu function, the input will be the output of the relu function.\n",
    "        This is because in practice we will have already performed this computation in the forward pass\n",
    "        so in the backward pass, we need to find its derivative drawing upon the cached relu(x).\n",
    "        \"\"\"\n",
    "        return 1 if x > 0.0 else 0.0\n",
    "    \n",
    "    def _binary_cross_ent(self, y_hat, y):\n",
    "        \"\"\"\n",
    "        This basically finds the negative of the log probability of class1 - its inverse\n",
    "        \"\"\"\n",
    "        return (-y * np.log(y_hat)) - ((1 - y) * np.log(1 - y_hat))\n",
    "    \n",
    "    def _negative_log_likelihood(self, y_hat):\n",
    "        return -np.log(y_hat)\n",
    "    \n",
    "    def _derivative_negative_log_likelihood(self, y_hat):\n",
    "        return -1/y_hat\n",
    "    \n",
    "    def _derivative_binary_cross_ent(self, y_hat, y):\n",
    "        \"\"\"\n",
    "        Derivative of binary cross-entropy\n",
    "        \n",
    "        This description is misleading. \n",
    "        This is the part of the partial derivative of binary cross-entropy \n",
    "        w.r.t the parameters of our function. In practice, the other part is \n",
    "        the dot product of this and the activations (activate(w, x))\n",
    "        \"\"\"\n",
    "        #return -(y / y_hat) - ((1 - y) / (1 - y_hat))\n",
    "        return (y_hat - y)\n",
    "\n",
    "    def _activate(self, x):\n",
    "        \"\"\"\n",
    "        RELU for non-linear activation function\n",
    "        \"\"\"\n",
    "        return self._relu(x)\n",
    "    \n",
    "    def _activate_vector(self, X):\n",
    "        \"\"\"\n",
    "        Run on a numpy vector\n",
    "        \"\"\"\n",
    "        activations = np.vectorize(self._activate)\n",
    "        return activations(X)\n",
    "    \n",
    "    def _derivative_activation(self, x):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the activation function given the activation output\n",
    "        \n",
    "        x: activate(node)\n",
    "        \"\"\"\n",
    "        return self._derivative_relu(x)\n",
    "    \n",
    "    def _derivative_vector_activation(self, X):\n",
    "        \"\"\"\n",
    "        Derivative for each scalar in a numpy vector\n",
    "        \"\"\"\n",
    "        derivative_activations = np.vectorize(self._derivative_activation)\n",
    "        return derivative_activations(X)\n",
    "\n",
    "    def _loss(self, y_hat, y):\n",
    "        \"\"\"\n",
    "        y_hat: sofmax vector\n",
    "        y:     one-hot vector for the target\n",
    "        \n",
    "        Here we will plug in the negative_log_likelihood\n",
    "        in order to be able to compare the proability of our output at\n",
    "        the correct class, to 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Get the index of the correct class \n",
    "        (numpy will return a tuple of the index in each dimension)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the first (and only) dim, and the first (and only) index\n",
    "        i = np.where(y==1)[0][0]\n",
    "        return self._negative_log_likelihood(y_hat[i])\n",
    "    \n",
    "    def _derivative_loss(self, y_hat, y):\n",
    "        \"\"\"\n",
    "        This will be used in backprop for finding L'(output_layer_node)\n",
    "        \"\"\"\n",
    "        # Get the first (and only) dim, and the first (and only) index\n",
    "        i = np.where(y==1)[0][0]\n",
    "        return self._derivative_negative_log_likelihood(y_hat[i])\n",
    "    \n",
    "    def _targets_to_one_hots(self, targets):\n",
    "        \"\"\"\n",
    "        Interpret a vector of targets into a matrix\n",
    "        of one-hot representations\n",
    "        \"\"\"\n",
    "        # Get the number of unique target classes\n",
    "        num_classes = len(set(targets))\n",
    "        # Instantiate a matrix of one-hot vectors\n",
    "        # with one row per target, and one col per class\n",
    "        one_hots = np.zeros((len(targets), num_classes))\n",
    "        for i, one_hot in enumerate(one_hots):\n",
    "            # Set the one-hot vector to hae a 1 at its corresponding target slot\n",
    "            t = targets[i]\n",
    "            one_hot[t] = 1\n",
    "            \n",
    "        return one_hots\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass: Calculate the activations of each neuron\n",
    "        \"\"\"\n",
    "        if len(inputs) != self.input_size:\n",
    "          raise Exception(\"That is not the size of the input layer... try %i\" % self.input_size)\n",
    "        \n",
    "        # Set input activations, no need to actually calculate anything\n",
    "        for i, input in enumerate(inputs):\n",
    "            self.activations_in[i] = input\n",
    "        \n",
    "        # calculate the activations for each hidden layer\n",
    "        for h_layer_i in range(self.hidden_layers):\n",
    "            # Need to take previous layer activation value * weights for a given layer\n",
    "            # Starting with input layer X first hidden layer\n",
    "            if h_layer_i == 0:\n",
    "                \"\"\"\n",
    "                # Loop over the layer\n",
    "                for h_dim_j in range(self.hidden_size):\n",
    "                    # Loop over neurons in the layer before it\n",
    "                    for k in range(self.input_size):\n",
    "                        # Sum [f_k * w_k_j for k in input layer]\n",
    "                        self.activations_hidden[h_layer_i][h_dim_j] += self.activations_in[k] * self.weights_in[k][h_dim_j]\n",
    "                    # h(sum from above), aka run the nonlinear activation function\n",
    "                    self.activations_hidden[h_layer_i][h_dim_j] = self._activate(self.activations_hidden[h_layer_i][h_dim_j])\n",
    "                \"\"\"\n",
    "                # multiply the previous layer's activations by its weight vector for this layer's activations\n",
    "                self.activations_hidden[h_layer_i] = self.activations_in.T.dot(self.weights_in)\n",
    "                # Reset bias activation to 1.0\n",
    "                self.activations_hidden[h_layer_i][-1] = 1.0\n",
    "            else:\n",
    "                if h_layer_i == 0:\n",
    "                    # Loop over the layer\n",
    "                    for h_dim_j in range(self.hidden_size):\n",
    "                        # Loop over neurons in the layer before it\n",
    "                        for k in range(self.hidden_size):\n",
    "                            # Sum [f_k * w_k_j for k in previous hidden layer]\n",
    "                            self.activations_hidden[h_layer_i][h_dim_j] += self.activations_hidden[h_layer_i - 1][k]\\\n",
    "                            * self.weights_hidden[h_layer_i][k][h_dim_j]\n",
    "                        # h(sum from above), aka run the nonlinear activation function\n",
    "                        self.activations_hidden[h_layer_i][h_dim_j] = self.activations_hidden[h_layer_i][h_dim_j]\n",
    "                # multiply the previous layer's activations by its weight vector for this layer's activations\n",
    "                #self.activations_hidden[h_layer_i] = self._activate_vector(self.activations_hidden[h_layer_i - 1].T.dot(self.weights_hidden[h_layer_i - 1]))\n",
    "                # Reset bias activation to 1.0\n",
    "                #self.activations_hidden[h_layer_i][-1] = 1.0\n",
    "\n",
    "                \n",
    "        # Output activations will be the dot product of the final hidden layer, and the output weights\n",
    "        \"\"\"\n",
    "        for i in range(self.output_size):\n",
    "            for j in range(self.hidden_size):\n",
    "                print(self.weights_out)\n",
    "                print(self.weights_out[j][i])\n",
    "                self.activations_out[i] += self.activations_hidden[-1][j] * self.weights_out[j][i]\n",
    "            # h(sum from above), aka run the nonlinear activation function\n",
    "            self.activations_out[i] = self._activate(self.activations_out[i])\n",
    "        \"\"\"\n",
    "        # Activa\n",
    "        self.activations_out = self._activate(self.activations_hidden[-1]).T.dot(self.weights_out)\n",
    "        # [:-1] because this one should not have a bias parameter\n",
    "        #self.activations_out = self._activate_vector(self.activations_out)\n",
    "        \n",
    "        #Print all of the weights, to see updates\n",
    "        \"\"\"\n",
    "        print(\"ACTIVATIONS\")\n",
    "        print(self.activations_in)\n",
    "        print(self.activations_hidden)\n",
    "        print(self.activations_out)\n",
    "        \"\"\"\n",
    "        print(\"WEIGHTS:\")\n",
    "        print(self.weights_in)\n",
    "        print(self.weights_hidden)\n",
    "        print(self.weights_out)\n",
    "\n",
    "    def backward(self, targets):\n",
    "        \"\"\"\n",
    "        Backpropogation for finding the partial derivative of the each node w.r.t the loss function,\n",
    "        and updating weights based on those gradients\n",
    "        \"\"\"\n",
    "        if len(targets) != len(self.activations_out):\n",
    "            raise Exception(\"Your labels are not the same size as your output layer!\")\n",
    "        # Calculate loss - there will be a value for each node in the output layer\n",
    "        # Take the simoid of the activations of the output layer, because we are doing 2 class classification\n",
    "        # ***If we have >2 classes, we would use softmax***\n",
    "        \"\"\"\n",
    "        print(\"ACTIVATIONS IN\")\n",
    "        print(self.activations_in)\n",
    "        \n",
    "        print(\"ACTIVATIONS HIDDEN\")\n",
    "        print(self.activations_hidden)\n",
    "        \n",
    "        print(\"ACTIVATIONS OUT\")\n",
    "        print(self.activations_out)\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"SOFTMAX_LAYER\")\n",
    "        print(self._softmax(self.activations_out))\n",
    "        \"\"\"\n",
    "        print(\"TARGETS\")\n",
    "        print(targets)\n",
    "        \"\"\"\n",
    "        loss = self._loss(self._softmax(self.activations_out), targets)\n",
    "        \n",
    "        \"\"\"\n",
    "        Now we need to calculate the partial derivative of the loss w.r.t each weight.\n",
    "        Think of this as finding the amount that each node contributes to a change in the final loss.\n",
    "        \n",
    "        Each node has a value \"delta\", which represents the partial derivative of the loss w.r.t. its value:\n",
    "        Use the partial derivative of the loss function, in our case binary cross-entropy\n",
    "        \"\"\"\n",
    "        self.deltas_out = np.zeros([self.output_size])\n",
    "        derivative_loss = self._derivative_loss(self._softmax(self.activations_out), targets)\n",
    "        print(\"LOSS PER INSTANCE: %2f\" % loss)\n",
    "        print(\"DERIVATIVE LOSS: %2f\" % derivative_loss)\n",
    "        print(\"DERIVATIVE ACTIVATIONS:\")\n",
    "        print(self._derivative_vector_activation(self.activations_out))\n",
    "        for i, activation_out in enumerate(self.activations_out):\n",
    "            self.deltas_out[i] = derivative_loss * self._derivative_activation(activation_out)\n",
    "        \n",
    "        \"\"\"\n",
    "        Find derivative of activation (activation was found in the forward pass) * derivative of the inner function,\n",
    "        which is the parameter w\n",
    "        \"\"\"\n",
    "        self.Deltas_hidden = np.zeros([self.hidden_layers, self.hidden_size + 1])\n",
    "        ##############\n",
    "        #####TODO Needs to go in reverse if we have multiple hidden\n",
    "        for h_layer_i in range(self.hidden_layers):\n",
    "            # If it is the last hidden layer, then we look at the activations and deltas\n",
    "            # of the output layer, not the next hidden layer\n",
    "            print(\"LAYER: %i\" % h_layer_i)\n",
    "            if h_layer_i == self.hidden_layers - 1:\n",
    "                # Loop over each in hidden activation, +1 for bias\n",
    "                for h_dim_j in range(self.hidden_size + 1):\n",
    "                    for k, delta_out in enumerate(self.deltas_out):\n",
    "                        self.Deltas_hidden[h_layer_i][h_dim_j] += delta_out * self._derivative_activation(self.activations_out[k]) * self.weights_out[h_dim_j][k]\n",
    "            else:\n",
    "                # Do the same to find the hidden deltas\n",
    "                for h_dim_j in range(self.hidden_size + 1):\n",
    "                    for k, delta_h in enumerate(self.Deltas_hidden[h_layer_i + 1]):\n",
    "                        self.Deltas_hidden[h_layer_i][h_dim_j] += delta_h * self._derivative_activation(self.activations_hidden[h_layer_i + 1][k]) * self.weights_hidden[h_layer_i][h_dim_j][k]\n",
    "                        \n",
    "        \"\"\"\n",
    "        Now just do the same for L'(input layer)\n",
    "        \"\"\"\n",
    "        self.deltas_in = np.zeros([self.input_size+1])\n",
    "        #### Need deltas_hidden[0].dot(der_act_hidden[0] * weights_in)\n",
    "        # Loop over each in input activation, +1 for bias\n",
    "        #for dim_i in range(self.input_size + 1):\n",
    "        #    for k, delta_h in enumerate(self.Deltas_hidden[0]):\n",
    "        #        self.deltas_in[dim_i] += delta_h * self._derivative_activation(self.activations_hidden[0][k]) * self.weights_in[dim_i][k]\n",
    "        \n",
    "        self.update_weights()\n",
    "        return loss\n",
    "        \n",
    "    def update_weights(self):\n",
    "        print(\"UPDATING WEIGHTS\")\n",
    "        # Now we can use the deltas to adjust each weight by L'(w_i_j)\n",
    "        # These weights are the edges shared between last hidden layer, and output layer\n",
    "        # Rows of weights_out correponds with length of last hidden layer\n",
    "        for i in range(len(self.weights_out)):\n",
    "            # Cols of weights_out correspinds with length of output layer\n",
    "            for j in range(len(self.weights_out[i])):\n",
    "                self.weights_out[i][j] -= self._activate(self.activations_hidden[-1][i])\\\n",
    "                * self._derivative_activation(self.activations_out[j])\\\n",
    "                * self.deltas_out[j]\\\n",
    "                * self.learning_rate\n",
    "                    \n",
    "        # Loop over each hidden layer\n",
    "        for w_i in reversed(range(len(self.weights_hidden))):\n",
    "            # Rows (i) in the weights for this layer will correspond to the size of the layer BEFORE (hidden or input)\n",
    "            for i in range(len(self.weights_hidden[w_i])):\n",
    "                # Cols (j) in these weights will correspond to the size of hidden layer w_i\n",
    "                for j in range(len(self.weights_hidden[w_i][i])):\n",
    "                    # We do not want to update the dummy 0 weight going TO the extra bias dimension\n",
    "                    if j == len(self.weights_hidden[w_i][i]) - 1:\n",
    "                        continue\n",
    "                    else:\n",
    "                        # + 1 for layer before, because we are looping in reverse\n",
    "                        self.weights_hidden[w_i][i][j] -= self._activate(self.activations_hidden[w_i + 1][i])\\\n",
    "                        * self._derivative_activation(self.activations_hidden[w_i][j])\\\n",
    "                        * self.Deltas_hidden[w_i][j]\\\n",
    "                        * self.learning_rate\n",
    "                        \n",
    "            #self.weights_hidden[w_i] -= self.activations_hidden[w_i].T.dot(self.Deltas_hidden[w_i]) * self.learning_rate\n",
    "        # Rows (i) of weights_in corresponds to size of the input layer\n",
    "        for i in range(len(self.weights_in)):\n",
    "            # Cols (j) corresponds to size of the first hidden layer (layer above input layer)\n",
    "            for j in range(len(self.weights_in[i])):\n",
    "                # We do not want to update the dummy 0 weight going TO the extra bias dimension\n",
    "                if j == len(self.weights_in[i]) - 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    \"\"\"\n",
    "                    print(\"act, deriv_act, delta, weight update val\")\n",
    "                    print(self.activations_in[i])\n",
    "                    print(self._derivative_activation(self.activations_hidden[0][j]))\n",
    "                    print(self.Deltas_hidden[0][j])\n",
    "                    print(self.activations_in[i]\\\n",
    "                    * self._derivative_activation(self.activations_hidden[0][j])\\\n",
    "                    * self.Deltas_hidden[0][j]\\\n",
    "                    * self.learning_rate)\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    self.weights_in[i][j] -= self.activations_in[i]\\\n",
    "                    * self._derivative_activation(self.activations_hidden[0][j])\\\n",
    "                    * self.Deltas_hidden[0][j]\\\n",
    "                    * self.learning_rate\n",
    "    \n",
    "    def train(self, inputs, targets, epochs=50, lr=.01):\n",
    "        self.learning_rate = lr\n",
    "        one_hot_targets = self._targets_to_one_hots(targets)\n",
    "        for e in range(epochs):\n",
    "            print(\"EPOCH %i\" % e)\n",
    "            \"\"\"\n",
    "            SGD - randomize the order of the training samples, and \n",
    "            \"\"\"\n",
    "            in_out = list(zip(inputs, one_hot_targets))\n",
    "            random.shuffle(in_out)\n",
    "            # For tracking average loss over SGD, just for logging\n",
    "            losses = []\n",
    "            \n",
    "            for inp, target in in_out:\n",
    "                if inp == [-1, -1]:\n",
    "                    print(\"TARGET: \")\n",
    "                    print(target)\n",
    "                    self.forward(inp)\n",
    "                    losses.append(self.backward(target))\n",
    "\n",
    "            print(\"LOSS: %2f\" % (sum(losses)/len(losses)))\n",
    "            \n",
    "\"\"\"\n",
    "Note that the 4th param, the size of the output layer, should be the\n",
    "number of classes\n",
    "\"\"\"\n",
    "#MLP = NN(2, 1, 2, 2)\n",
    "#xor_inputs = [[1, 1], [-1, 1], [-1, -1], [1, -1]]\n",
    "#xor_labels = ([1, 0, 1, 0])\n",
    "#MLP.train(xor_inputs, xor_labels)\n",
    "\"\"\"\n",
    "class Node(Object):\n",
    "    def __init__(self):\n",
    "        \n",
    "    def \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.08035993  0.76871217  0.        ]\n",
      " [ 1.24510298 -1.60994973  0.        ]\n",
      " [ 0.85942042 -0.9706533   0.        ]]\n",
      "[]\n",
      "[[ 0.6264084   0.5410394 ]\n",
      " [ 0.94291019  0.4660081 ]\n",
      " [-0.3151102  -1.63519543]]\n",
      "SOFTMAX_LAYER\n",
      "[0.77422113781025936, 0.22577886218974061]\n",
      "LOSS PER INSTANCE: 1.488199\n",
      "DERIVATIVE LOSS: -4.429113\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.2268579  0.1164989]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.488199\n",
      "EPOCH 1\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.08421384  0.76326881  0.        ]\n",
      " [ 1.24124907 -1.61539309  0.        ]\n",
      " [ 0.86327433 -0.96520994  0.        ]]\n",
      "[]\n",
      "[[ 0.63511194  0.54333466]\n",
      " [ 0.95250478  0.46853834]\n",
      " [-0.30011268 -1.63124035]]\n",
      "SOFTMAX_LAYER\n",
      "[0.7772024616490083, 0.22279753835099159]\n",
      "LOSS PER INSTANCE: 1.501492\n",
      "DERIVATIVE LOSS: -4.488380\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.22915623  0.11793918]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.501492\n",
      "EPOCH 2\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.08825731  0.75757757  0.        ]\n",
      " [ 1.2372056  -1.62108433  0.        ]\n",
      " [ 0.8673178  -0.95951869  0.        ]]\n",
      "[]\n",
      "[[ 0.64417152  0.54573438]\n",
      " [ 0.96251204  0.47118909]\n",
      " [-0.28460497 -1.62713263]]\n",
      "SOFTMAX_LAYER\n",
      "[0.78033709749623237, 0.21966290250376772]\n",
      "LOSS PER INSTANCE: 1.515661\n",
      "DERIVATIVE LOSS: -4.552430\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.23147936  0.11946764]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.515661\n",
      "EPOCH 3\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.09250584  0.75161976  0.        ]\n",
      " [ 1.23295707 -1.62704214  0.        ]\n",
      " [ 0.87156633 -0.95356088  0.        ]]\n",
      "[]\n",
      "[[ 0.65361285  0.54824922]\n",
      " [ 0.97296241  0.47397269]\n",
      " [-0.26855544 -1.62285761]]\n",
      "SOFTMAX_LAYER\n",
      "[0.78363578311612248, 0.2163642168838775]\n",
      "LOSS PER INSTANCE: 1.530792\n",
      "DERIVATIVE LOSS: -4.621836\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.23381394  0.12109291]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.530792\n",
      "EPOCH 4\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.09697641  0.74537516  0.        ]\n",
      " [ 1.2284865  -1.63328674  0.        ]\n",
      " [ 0.8760369  -0.94731628  0.        ]]\n",
      "[]\n",
      "[[ 0.66346355  0.55089141]\n",
      " [ 0.9838886   0.47690335]\n",
      " [-0.25193088 -1.61839851]]\n",
      "SOFTMAX_LAYER\n",
      "[0.78710979053657315, 0.21289020946342682]\n",
      "LOSS PER INSTANCE: 1.546979\n",
      "DERIVATIVE LOSS: -4.697257\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.23614235  0.12282473]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.546979\n",
      "EPOCH 5\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.1016876   0.738822    0.        ]\n",
      " [ 1.22377531 -1.6398399   0.        ]\n",
      " [ 0.88074809 -0.94076313  0.        ]]\n",
      "[]\n",
      "[[ 0.67375309  0.55367509]\n",
      " [ 0.99532564  0.47999748]\n",
      " [-0.23469685 -1.6137361 ]]\n",
      "SOFTMAX_LAYER\n",
      "[0.79077079498227498, 0.20922920501772499]\n",
      "LOSS PER INSTANCE: 1.564325\n",
      "DERIVATIVE LOSS: -4.779447\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.2384415   0.12467404]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.564325\n",
      "EPOCH 6\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.10665964  0.73193699  0.        ]\n",
      " [ 1.21880327 -1.6467249   0.        ]\n",
      " [ 0.88572013 -0.93387812  0.        ]]\n",
      "[]\n",
      "[[ 0.68451262  0.55661667]\n",
      " [ 1.00731062  0.4832741 ]\n",
      " [-0.21681815 -1.60884818]]\n",
      "SOFTMAX_LAYER\n",
      "[0.79463066069396726, 0.20536933930603271]\n",
      "LOSS PER INSTANCE: 1.582945\n",
      "DERIVATIVE LOSS: -4.869276\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.24068148  0.12665313]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.582945\n",
      "EPOCH 7\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.11191446  0.72469546  0.        ]\n",
      " [ 1.21354845 -1.65396643  0.        ]\n",
      " [ 0.89097495 -0.92663659  0.        ]]\n",
      "[]\n",
      "[[ 0.69577468  0.55973531]\n",
      " [ 1.01988241  0.48675541]\n",
      " [-0.19825958 -1.60370903]]\n",
      "SOFTMAX_LAYER\n",
      "[0.79870111130785693, 0.20129888869214305]\n",
      "LOSS PER INSTANCE: 1.602964\n",
      "DERIVATIVE LOSS: -4.967737\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.24282401  0.12877582]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.602964\n",
      "EPOCH 8\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.11747561  0.71707161  0.        ]\n",
      " [ 1.2079873  -1.66159029  0.        ]\n",
      " [ 0.8965361  -0.91901274  0.        ]]\n",
      "[]\n",
      "[[ 0.70757261  0.56305342]\n",
      " [ 1.03308096  0.49046744]\n",
      " [-0.17898715 -1.59828875]]\n",
      "SOFTMAX_LAYER\n",
      "[0.80299324294797902, 0.19700675705202106]\n",
      "LOSS PER INSTANCE: 1.624517\n",
      "DERIVATIVE LOSS: -5.075968\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.24482046  0.13105761]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.624517\n",
      "EPOCH 9\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.12336813  0.70903897  0.        ]\n",
      " [ 1.20209478 -1.66962293  0.        ]\n",
      " [ 0.90242862 -0.91098009  0.        ]]\n",
      "[]\n",
      "[[ 0.71993966  0.56659743]\n",
      " [ 1.04694628  0.49444081]\n",
      " [-0.15896969 -1.59255237]]\n",
      "SOFTMAX_LAYER\n",
      "[0.80751682768904642, 0.19248317231095352]\n",
      "LOSS PER INSTANCE: 1.647747\n",
      "DERIVATIVE LOSS: -5.195259\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.24660986  0.13351581]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.647747\n",
      "EPOCH 10\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.12961819  0.70057108  0.        ]\n",
      " [ 1.19584472 -1.67809081  0.        ]\n",
      " [ 0.90867868 -0.90251221  0.        ]]\n",
      "[]\n",
      "[[ 0.73290765  0.5703986 ]\n",
      " [ 1.06151691  0.49871175]\n",
      " [-0.13818121 -1.58645886]]\n",
      "SOFTMAX_LAYER\n",
      "[0.81227934515798073, 0.18772065484201933]\n",
      "LOSS PER INSTANCE: 1.672800\n",
      "DERIVATIVE LOSS: -5.327064\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.24811667  0.13616968]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.672800\n",
      "EPOCH 11\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.13625255  0.69164263  0.        ]\n",
      " [ 1.18921036 -1.68701927  0.        ]\n",
      " [ 0.91531304 -0.89358375  0.        ]]\n",
      "[]\n",
      "[[ 0.74650503  0.57449408]\n",
      " [ 1.0768276   0.50332326]\n",
      " [-0.11660403 -1.57995991]]\n",
      "SOFTMAX_LAYER\n",
      "[0.81728467406502148, 0.18271532593497858]\n",
      "LOSS PER INSTANCE: 1.699826\n",
      "DERIVATIVE LOSS: -5.472995\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.24924879  0.13904046]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.699826\n",
      "EPOCH 12\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.1432977   0.68223085  0.        ]\n",
      " [ 1.18216521 -1.69643105  0.        ]\n",
      " [ 0.92235819 -0.88417197  0.        ]]\n",
      "[]\n",
      "[[ 0.76075411  0.57892815]\n",
      " [ 1.0929061   0.50832662]\n",
      " [-0.09423301 -1.57299842]]\n",
      "SOFTMAX_LAYER\n",
      "[0.82253137938258924, 0.17746862061741078]\n",
      "LOSS PER INSTANCE: 1.728961\n",
      "DERIVATIVE LOSS: -5.634799\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.2498962   0.14215137]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.728961\n",
      "EPOCH 13\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.15077861  0.67231759  0.        ]\n",
      " [ 1.1746843  -1.7063443   0.        ]\n",
      " [ 0.9298391  -0.87425872  0.        ]]\n",
      "[]\n",
      "[[ 0.77566742  0.58375381]\n",
      " [ 1.10976884  0.51378308]\n",
      " [-0.07108081 -1.56550681]]\n",
      "SOFTMAX_LAYER\n",
      "[0.8280105543384797, 0.1719894456615203]\n",
      "LOSS PER INSTANCE: 1.760322\n",
      "DERIVATIVE LOSS: -5.814310\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.24993087  0.14552742]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.760322\n",
      "EPOCH 14\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.15871701  0.66189192  0.        ]\n",
      " [ 1.1667459  -1.71676998  0.        ]\n",
      " [ 0.9377775  -0.86383305  0.        ]]\n",
      "[]\n",
      "[[ 0.79124299  0.58903454]\n",
      " [ 1.12741538  0.51976595]\n",
      " [-0.04718439 -1.55740499]]\n",
      "SOFTMAX_LAYER\n",
      "[0.83370323275294633, 0.16629676724705364]\n",
      "LOSS PER INSTANCE: 1.793981\n",
      "DERIVATIVE LOSS: -6.013346\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.24920901  0.14919507]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.793981\n",
      "EPOCH 15\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.1671293   0.65095318  0.        ]\n",
      " [ 1.15833362 -1.72770872  0.        ]\n",
      " [ 0.94618979 -0.85289431  0.        ]]\n",
      "[]\n",
      "[[ 0.80745861  0.5948464 ]\n",
      " [ 1.14582152  0.52636292]\n",
      " [-0.02261251 -1.54859815]]\n",
      "SOFTMAX_LAYER\n",
      "[0.83957749144258476, 0.16042250855741533]\n",
      "LOSS PER INSTANCE: 1.829944\n",
      "DERIVATIVE LOSS: -6.233539\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.24757677  0.15318161]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.829944\n",
      "EPOCH 16\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.176024    0.63951442  0.        ]\n",
      " [ 1.14943891 -1.73914748  0.        ]\n",
      " [ 0.95508449 -0.84145554  0.        ]]\n",
      "[]\n",
      "[[ 0.8242653   0.60128032]\n",
      " [ 1.16493162  0.53367862]\n",
      " [ 0.00252655 -1.53897444]]\n",
      "SOFTMAX_LAYER\n",
      "[0.84558552365923367, 0.15441447634076635]\n",
      "LOSS PER INSTANCE: 1.868115\n",
      "DERIVATIVE LOSS: -6.476077\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.24488064  0.15751428]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.868115\n",
      "EPOCH 17\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.18539926  0.62760563  0.        ]\n",
      " [ 1.14006365 -1.75105626  0.        ]\n",
      " [ 0.96445975 -0.82954676  0.        ]]\n",
      "[]\n",
      "[[ 0.84158079  0.60844449]\n",
      " [ 1.18465074  0.54183728]\n",
      " [ 0.02807801 -1.5284027 ]]\n",
      "SOFTMAX_LAYER\n",
      "[0.85166117181143641, 0.14833882818856364]\n",
      "LOSS PER INSTANCE: 1.908256\n",
      "DERIVATIVE LOSS: -6.741323\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.24098321  0.16221909]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.908256\n",
      "EPOCH 18\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.19524066  0.61527611  0.        ]\n",
      " [ 1.13022225 -1.76338579  0.        ]\n",
      " [ 0.97430115 -0.81721724  0.        ]]\n",
      "[]\n",
      "[[ 0.85928404  0.6164665 ]\n",
      " [ 1.20483808  0.55098493]\n",
      " [ 0.05383608 -1.51673074]]\n",
      "SOFTMAX_LAYER\n",
      "[0.8577186094530278, 0.14228139054697225]\n",
      "LOSS PER INSTANCE: 1.949949\n",
      "DERIVATIVE LOSS: -7.028326\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.23578346  0.16731936]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.949949\n",
      "EPOCH 19\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.20552025  0.60259481  0.        ]\n",
      " [ 1.11994266 -1.77606709  0.        ]\n",
      " [ 0.98458074 -0.80453594  0.        ]]\n",
      "[]\n",
      "[[ 0.8772126   0.62549491]\n",
      " [ 1.22530378  0.56129098]\n",
      " [ 0.07954437 -1.50378464]]\n",
      "SOFTMAX_LAYER\n",
      "[0.86365295251594987, 0.13634704748405016]\n",
      "LOSS PER INSTANCE: 1.992552\n",
      "DERIVATIVE LOSS: -7.334226\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.22923895  0.172834  ]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.992552\n",
      "EPOCH 20\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.21619752  0.58964787  0.        ]\n",
      " [ 1.10926539 -1.78901403  0.        ]\n",
      " [ 0.99525801 -0.79158899  0.        ]]\n",
      "[]\n",
      "[[ 0.89516453  0.63569943]\n",
      " [ 1.24581112  0.57294809]\n",
      " [ 0.104903   -1.48936988]]\n",
      "SOFTMAX_LAYER\n",
      "[0.8693434157900185, 0.13065658420998155]\n",
      "LOSS PER INSTANCE: 2.035183\n",
      "DERIVATIVE LOSS: -7.653652\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.22138452  0.17877549]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 2.035183\n",
      "EPOCH 21\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.22722276  0.57653253  0.        ]\n",
      " [ 1.09824016 -1.80212937  0.        ]\n",
      " [ 1.00628325 -0.77847365  0.        ]]\n",
      "[]\n",
      "[[ 0.91290639  0.64726909]\n",
      " [ 1.266086    0.58616956]\n",
      " [ 0.12958373 -1.47327531]]\n",
      "SOFTMAX_LAYER\n",
      "[0.87465911335454982, 0.12534088664545007]\n",
      "LOSS PER INSTANCE: 2.076718\n",
      "DERIVATIVE LOSS: -7.978243\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.21234106  0.18514738]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 2.076718\n",
      "EPOCH 22\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.23854286  0.56334795  0.        ]\n",
      " [ 1.08692005 -1.81531395  0.        ]\n",
      " [ 1.01760335 -0.76528908  0.        ]]\n",
      "[]\n",
      "[[ 0.93018735  0.66040727]\n",
      " [ 1.28583366  0.60118309]\n",
      " [ 0.15325218 -1.45528093]]\n",
      "SOFTMAX_LAYER\n",
      "[0.87946680058849958, 0.12053319941150038]\n",
      "LOSS PER INSTANCE: 2.115830\n",
      "DERIVATIVE LOSS: -8.296469\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.20230931  0.19194044]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 2.115830\n",
      "EPOCH 23\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.25010845  0.5501844   0.        ]\n",
      " [ 1.07535446 -1.8284775   0.        ]\n",
      " [ 1.02916894 -0.75212553  0.        ]]\n",
      "[]\n",
      "[[ 0.94675771  0.6753226 ]\n",
      " [ 1.3047608   0.61821981]\n",
      " [ 0.17559406 -1.43517051]]\n",
      "SOFTMAX_LAYER\n",
      "[0.88363908650971046, 0.11636091349028949]\n",
      "LOSS PER INSTANCE: 2.151059\n",
      "DERIVATIVE LOSS: -8.593951\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.19154783  0.19912642]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 2.151059\n",
      "EPOCH 24\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.26188033  0.53711338  0.        ]\n",
      " [ 1.06358258 -1.84154852  0.        ]\n",
      " [ 1.04094082 -0.7390545   0.        ]]\n",
      "[]\n",
      "[[ 0.96238874  0.69221498]\n",
      " [ 1.32259922  0.63749772]\n",
      " [ 0.19634043 -1.41275001]]\n",
      "SOFTMAX_LAYER\n",
      "[0.88706136550061498, 0.11293863449938503]\n",
      "LOSS PER INSTANCE: 2.180911\n",
      "DERIVATIVE LOSS: -8.854366\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.18034047  0.20664771]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 2.180911\n",
      "EPOCH 25\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.27383307  0.52418164  0.        ]\n",
      " [ 1.05162984 -1.85448026  0.        ]\n",
      " [ 1.05289356 -0.72612277  0.        ]]\n",
      "[]\n",
      "[[ 0.97689003  0.71125562]\n",
      " [ 1.33912647  0.65919849]\n",
      " [ 0.21528735 -1.38787213]]\n",
      "SOFTMAX_LAYER\n",
      "[0.88963622763788519, 0.11036377236211477]\n",
      "LOSS PER INSTANCE: 2.203973\n",
      "DERIVATIVE LOSS: -9.060944\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.1689627   0.21440141]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 2.203973\n",
      "EPOCH 26\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.28595357  0.51141123  0.        ]\n",
      " [ 1.03950935 -1.86725067  0.        ]\n",
      " [ 1.06501406 -0.71335235  0.        ]]\n",
      "[]\n",
      "[[ 0.99012151  0.73256066]\n",
      " [ 1.35418008  0.68343748]\n",
      " [ 0.23230697 -1.36046754]]\n",
      "SOFTMAX_LAYER\n",
      "[0.89128528711218136, 0.10871471288781871]\n",
      "LOSS PER INSTANCE: 2.219028\n",
      "DERIVATIVE LOSS: -9.198387\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.15765614  0.22221789]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 2.219028\n",
      "EPOCH 27\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.2982335   0.49880648  0.        ]\n",
      " [ 1.02722941 -1.87985542  0.        ]\n",
      " [ 1.07729399 -0.7007476   0.        ]]\n",
      "[]\n",
      "[[ 1.00199924  0.75615835]\n",
      " [ 1.36766432  0.71022686]\n",
      " [ 0.24734975 -1.33058177]]\n",
      "SOFTMAX_LAYER\n",
      "[0.89194971273854229, 0.10805028726145768]\n",
      "LOSS PER INSTANCE: 2.225159\n",
      "DERIVATIVE LOSS: -9.254950\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.14661657  0.22983667]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 2.225159\n",
      "EPOCH 28\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.310656    0.48636741  0.        ]\n",
      " [ 1.01480691 -1.89229449  0.        ]\n",
      " [ 1.08971649 -0.68830854  0.        ]]\n",
      "[]\n",
      "[[ 1.01249544  0.78195151]\n",
      " [ 1.37954999  0.73943449]\n",
      " [ 0.2604396  -1.298415  ]]\n",
      "SOFTMAX_LAYER\n",
      "[0.89159165364353998, 0.1084083463564601]\n",
      "LOSS PER INSTANCE: 2.221850\n",
      "DERIVATIVE LOSS: -9.224382\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.13599544  0.23688857]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 2.221850\n",
      "EPOCH 29\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.32317859  0.47410719  0.        ]\n",
      " [ 1.00228432 -1.90455471  0.        ]\n",
      " [ 1.10223908 -0.67604832  0.        ]]\n",
      "[]\n",
      "[[ 1.02163451  0.80968099]\n",
      " [ 1.38986931  0.77074503]\n",
      " [ 0.27166445 -1.26435692]]\n",
      "SOFTMAX_LAYER\n",
      "[0.8901987499005567, 0.10980125009944337]\n",
      "LOSS PER INSTANCE: 2.209083\n",
      "DERIVATIVE LOSS: -9.107364\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.12591017  0.2428995 ]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 2.209083\n",
      "EPOCH 30\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.33571692  0.46206924  0.        ]\n",
      " [ 0.98974599 -1.91659265  0.        ]\n",
      " [ 1.11477741 -0.66401037  0.        ]]\n",
      "[]\n",
      "[[ 1.02948616  0.83890183]\n",
      " [ 1.39870729  0.80363659]\n",
      " [ 0.28116413 -1.22900275]]\n",
      "SOFTMAX_LAYER\n",
      "[0.88779262347429799, 0.11220737652570198]\n",
      "LOSS PER INSTANCE: 2.187407\n",
      "DERIVATIVE LOSS: -8.912070\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.11645644  0.24733597]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 2.187407\n",
      "EPOCH 31\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.34813647  0.45033759  0.        ]\n",
      " [ 0.97732644 -1.92832431  0.        ]\n",
      " [ 1.12719696 -0.65227871  0.        ]]\n",
      "[]\n",
      "[[ 1.03615645  0.86898974]\n",
      " [ 1.4061907   0.8373923 ]\n",
      " [ 0.28911657 -1.19313139]]\n",
      "SOFTMAX_LAYER\n",
      "[0.88443946443543731, 0.11556053556456256]\n",
      "LOSS PER INSTANCE: 2.157961\n",
      "DERIVATIVE LOSS: -8.653473\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.10771572  0.2497033 ]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 2.157961\n",
      "EPOCH 32\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.36025951  0.43903361  0.        ]\n",
      " [ 0.9652034  -1.93962828  0.        ]\n",
      " [ 1.13932    -0.64097474  0.        ]]\n",
      "[]\n",
      "[[ 1.04177695  0.89919381]\n",
      " [ 1.41247497  0.87116341]\n",
      " [ 0.29572265 -1.15763095]]\n",
      "SOFTMAX_LAYER\n",
      "[0.88025728460173758, 0.11974271539826237]\n",
      "LOSS PER INSTANCE: 2.122410\n",
      "DERIVATIVE LOSS: -8.351239\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.09975404  0.24967856]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 2.122410\n",
      "EPOCH 33\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.3718901   0.42829668  0.        ]\n",
      " [ 0.95357281 -1.95036522  0.        ]\n",
      " [ 1.15095059 -0.63023781  0.        ]]\n",
      "[]\n",
      "[[ 1.0464926   0.92873604]\n",
      " [ 1.41772971  0.90408285]\n",
      " [ 0.30119037 -1.12337721]]\n",
      "SOFTMAX_LAYER\n",
      "[0.87541268999798472, 0.12458731000201535]\n",
      "LOSS PER INSTANCE: 2.082749\n",
      "DERIVATIVE LOSS: -8.026500\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.09261324  0.24722205]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 2.082749\n",
      "EPOCH 34\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.38285016  0.41825425  0.        ]\n",
      " [ 0.94261275 -1.96040765  0.        ]\n",
      " [ 1.16191065 -0.62019538  0.        ]]\n",
      "[]\n",
      "[[ 1.05044936  0.95693077]\n",
      " [ 1.42212437  0.93539792]\n",
      " [ 0.30572004 -1.09110005]]\n",
      "SOFTMAX_LAYER\n",
      "[0.87010342043256517, 0.12989657956743481]\n",
      "LOSS PER INSTANCE: 2.041017\n",
      "DERIVATIVE LOSS: -7.698432\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.08630088  0.24260159]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 2.041017\n",
      "EPOCH 35\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.39301163  0.40899463  0.        ]\n",
      " [ 0.93245128 -1.96966727  0.        ]\n",
      " [ 1.17207212 -0.61093575  0.        ]]\n",
      "[]\n",
      "[[ 1.05378368  0.98327975]\n",
      " [ 1.42581626  0.96457258]\n",
      " [ 0.30949253 -1.06128852]]\n",
      "SOFTMAX_LAYER\n",
      "[0.86453099176927395, 0.13546900823072602]\n",
      "LOSS PER INSTANCE: 1.999012\n",
      "DERIVATIVE LOSS: -7.381762\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.08078655  0.23630977]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.999012\n",
      "EPOCH 36\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.40231168  0.4005537   0.        ]\n",
      " [ 0.92315123 -1.9781082   0.        ]\n",
      " [ 1.18137217 -0.60249483  0.        ]]\n",
      "[]\n",
      "[[ 1.05661541  1.0075088 ]\n",
      " [ 1.42894272  0.9913235 ]\n",
      " [ 0.31266234 -1.03416675]]\n",
      "SOFTMAX_LAYER\n",
      "[0.85887416176124975, 0.14112583823875016]\n",
      "LOSS PER INSTANCE: 1.958103\n",
      "DERIVATIVE LOSS: -7.085875\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.07600766  0.22892233]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.958103\n",
      "EPOCH 37\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.41074854  0.3929182   0.        ]\n",
      " [ 0.91471437 -1.9857437   0.        ]\n",
      " [ 1.18980903 -0.59485933  0.        ]]\n",
      "[]\n",
      "[[ 1.05904474  1.02954558]\n",
      " [ 1.43161797  1.01559098]\n",
      " [ 0.31535575 -1.00973444]]\n",
      "SOFTMAX_LAYER\n",
      "[0.85327262603766607, 0.14672737396233385]\n",
      "LOSS PER INSTANCE: 1.919179\n",
      "DERIVATIVE LOSS: -6.815361\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.07188176  0.22097114]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.919179\n",
      "EPOCH 38\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.41836514  0.38603925  0.        ]\n",
      " [ 0.90709777 -1.99262265  0.        ]\n",
      " [ 1.19742563 -0.58798037  0.        ]]\n",
      "[]\n",
      "[[ 1.06115248  1.04946384]\n",
      " [ 1.43393367  1.03747449]\n",
      " [ 0.31767272 -0.98783895]]\n",
      "SOFTMAX_LAYER\n",
      "[0.84782329485999652, 0.1521767051400035]\n",
      "LOSS PER INSTANCE: 1.882713\n",
      "DERIVATIVE LOSS: -6.571308\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.06831967  0.21287402]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.882713\n",
      "EPOCH 39\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.42523027  0.37984814  0.        ]\n",
      " [ 0.90023264 -1.99881376  0.        ]\n",
      " [ 1.20429076 -0.58178927  0.        ]]\n",
      "[]\n",
      "[[ 1.06300232  1.06742305]\n",
      " [ 1.4359618   1.05716468]\n",
      " [ 0.3196908  -0.96824636]]\n",
      "SOFTMAX_LAYER\n",
      "[0.84258548676802236, 0.15741451323197769]\n",
      "LOSS PER INSTANCE: 1.848873\n",
      "DERIVATIVE LOSS: -6.352654\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.06523547  0.20492017]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.848873\n",
      "EPOCH 40\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.43142313  0.37426919  0.        ]\n",
      " [ 0.89403978 -2.00439271  0.        ]\n",
      " [ 1.21048362 -0.57621032  0.        ]]\n",
      "[]\n",
      "[[ 1.06464383  1.08362044]\n",
      " [ 1.43775819  1.07489036]\n",
      " [ 0.32146956 -0.95069466]]\n",
      "SOFTMAX_LAYER\n",
      "[0.83758982552538674, 0.16241017447461326]\n",
      "LOSS PER INSTANCE: 1.817630\n",
      "DERIVATIVE LOSS: -6.157250\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.06255209  0.1972878 ]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.817630\n",
      "EPOCH 41\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.43702317  0.36922806  0.        ]\n",
      " [ 0.88843974 -2.00943384  0.        ]\n",
      " [ 1.21608366 -0.57116918  0.        ]]\n",
      "[]\n",
      "[[ 1.06611546  1.09825961]\n",
      " [ 1.43936602  1.09088431]\n",
      " [ 0.32305469 -0.93492646]]\n",
      "SOFTMAX_LAYER\n",
      "[0.83284712372113456, 0.16715287627886535]\n",
      "LOSS PER INSTANCE: 1.788846\n",
      "DERIVATIVE LOSS: -5.982547\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.06020339  0.19007263]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.788846\n",
      "EPOCH 42\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.44210449  0.36465626  0.        ]\n",
      " [ 0.88335842 -2.01400564  0.        ]\n",
      " [ 1.22116498 -0.56659738  0.        ]]\n",
      "[]\n",
      "[[ 1.0674471   1.1115331 ]\n",
      " [ 1.44081875  1.10536477]\n",
      " [ 0.32448136 -0.9207058 ]]\n",
      "SOFTMAX_LAYER\n",
      "[0.82835557623257561, 0.17164442376742434]\n",
      "LOSS PER INSTANCE: 1.762330\n",
      "DERIVATIVE LOSS: -5.825998\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.05813415  0.1833147 ]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.762330\n",
      "EPOCH 43\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.4467334   0.36049298  0.        ]\n",
      " [ 0.87872951 -2.01816892  0.        ]\n",
      " [ 1.22579389 -0.56243411  0.        ]]\n",
      "[]\n",
      "[[ 1.06866213  1.12361449]\n",
      " [ 1.44214249  1.11852721]\n",
      " [ 0.32577683 -0.9078245 ]]\n",
      "SOFTMAX_LAYER\n",
      "[0.82410594901424006, 0.17589405098576003]\n",
      "LOSS PER INSTANCE: 1.737873\n",
      "DERIVATIVE LOSS: -5.685241\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.05629893  0.17701921]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.737873\n",
      "EPOCH 44\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.45096778  0.35668541  0.        ]\n",
      " [ 0.87449513 -2.02197649  0.        ]\n",
      " [ 1.23002827 -0.55862653  0.        ]]\n",
      "[]\n",
      "[[ 1.06977896  1.13465599]\n",
      " [ 1.4433578   1.13054229]\n",
      " [ 0.32696244 -0.89610297]]\n",
      "SOFTMAX_LAYER\n",
      "[0.82008505637656703, 0.17991494362343291]\n",
      "LOSS PER INSTANCE: 1.715271\n",
      "DERIVATIVE LOSS: -5.558182\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.0546606   0.17117109]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.715271\n",
      "EPOCH 45\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.45485753  0.3531882   0.        ]\n",
      " [ 0.87060538 -2.0254737   0.        ]\n",
      " [ 1.23391802 -0.55512933  0.        ]]\n",
      "[]\n",
      "[[ 1.07081227  1.1447891 ]\n",
      " [ 1.444481    1.1415569 ]\n",
      " [ 0.32805508 -0.88538807]]\n",
      "SOFTMAX_LAYER\n",
      "[0.81627797086781306, 0.18372202913218691]\n",
      "LOSS PER INSTANCE: 1.694331\n",
      "DERIVATIVE LOSS: -5.443005\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.05318886  0.16574449]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.694331\n",
      "EPOCH 46\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.45844532  0.34996267  0.        ]\n",
      " [ 0.86701759 -2.02869923  0.        ]\n",
      " [ 1.23750581 -0.55190379  0.        ]]\n",
      "[]\n",
      "[[ 1.07177385  1.15412645]\n",
      " [ 1.4455252   1.15169649]\n",
      " [ 0.32906823 -0.87554996]]\n",
      "SOFTMAX_LAYER\n",
      "[0.81266936512519539, 0.18733063487480459]\n",
      "LOSS PER INSTANCE: 1.674880\n",
      "DERIVATIVE LOSS: -5.338155\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.05185896  0.16070873]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.674880\n",
      "EPOCH 47\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.46176756  0.34697588  0.        ]\n",
      " [ 0.86369535 -2.03168602  0.        ]\n",
      " [ 1.24082805 -0.54891701  0.        ]]\n",
      "[]\n",
      "[[ 1.07267329  1.16276424]\n",
      " [ 1.44650103  1.16106791]\n",
      " [ 0.3300128  -0.86647875]]\n",
      "SOFTMAX_LAYER\n",
      "[0.80924428723078212, 0.19075571276921796]\n",
      "LOSS PER INSTANCE: 1.656762\n",
      "DERIVATIVE LOSS: -5.242307\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.05065062  0.15603182]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.656762\n",
      "EPOCH 48\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.46485534  0.34419978  0.        ]\n",
      " [ 0.86060758 -2.03446212  0.        ]\n",
      " [ 1.24391583 -0.5461409   0.        ]]\n",
      "[]\n",
      "[[ 1.07351844  1.17078456]\n",
      " [ 1.4474172   1.16976219]\n",
      " [ 0.33089769 -0.85808138]]\n",
      "SOFTMAX_LAYER\n",
      "[0.80598857853135031, 0.19401142146864966]\n",
      "LOSS PER INSTANCE: 1.639838\n",
      "DERIVATIVE LOSS: -5.154336\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.04954712  0.15168248]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.639838\n",
      "EPOCH 49\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-0.46773518  0.34161042  0.        ]\n",
      " [ 0.85772773 -2.03705148  0.        ]\n",
      " [ 1.24679567 -0.54355155  0.        ]]\n",
      "[]\n",
      "[[ 1.07431582  1.1782576 ]\n",
      " [ 1.44828092  1.17785696]\n",
      " [ 0.33173023 -0.8502788 ]]\n",
      "SOFTMAX_LAYER\n",
      "[0.80288907188487313, 0.19711092811512695]\n",
      "LOSS PER INSTANCE: 1.623989\n",
      "DERIVATIVE LOSS: -5.073285\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ 0.0485346   0.14763121]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.623989\n"
     ]
    }
   ],
   "source": [
    "class SigmoidNN(NN):\n",
    "    def _activate(self, x):\n",
    "        \"\"\"\n",
    "        override RelU with sigmoid\n",
    "        \"\"\"\n",
    "        return self._sigmoid(x)\n",
    "    \n",
    "    def _derivative_activation(self, x):\n",
    "        \"\"\"\n",
    "        Override RelU' with Sigmoid'\n",
    "        \"\"\"\n",
    "        return self._derivative_sigmoid(x)\n",
    "    \n",
    "\"\"\"\n",
    "Run the same test with sigmoid\n",
    "\"\"\"\n",
    "MLP = SigmoidNN(2, 1, 2, 2)\n",
    "xor_inputs = [[1, 1], [-1, 1], [-1, -1], [1, -1]]\n",
    "xor_labels = ([1, 0, 1, 0])\n",
    "MLP.train(xor_inputs, xor_labels, epochs=50, lr=.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ 0.09773946 -0.8236054   0.        ]\n",
      " [ 0.42621244 -0.66779998  0.        ]\n",
      " [-2.14975304 -0.93959653  0.        ]]\n",
      "[]\n",
      "[[-0.84511763  0.27598692]\n",
      " [ 0.50696887  1.09852717]\n",
      " [-0.88857492  0.63128769]]\n",
      "SOFTMAX_LAYER\n",
      "[0.75973241513799716, 0.24026758486200278]\n",
      "LOSS PER INSTANCE: 1.426002\n",
      "DERIVATIVE LOSS: -4.162026\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[-1.72504215  0.75044268]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 1.426002\n",
      "EPOCH 1\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-5.33653148 -1.37781277  0.        ]\n",
      " [-5.0080585  -1.22200734  0.        ]\n",
      " [ 3.28451791 -0.38538916  0.        ]]\n",
      "[]\n",
      "[[-1.94922632  0.06703447]\n",
      " [ 1.06639408  1.20439833]\n",
      " [-0.03964791  0.79194703]]\n",
      "SOFTMAX_LAYER\n",
      "[3.7306204634069575e-13, 0.99999999999962708]\n",
      "LOSS PER INSTANCE: 0.000000\n",
      "DERIVATIVE LOSS: -1.000000\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[-586.7913026   -18.11984055]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: 0.000000\n",
      "EPOCH 2\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[-11159599.13033338    129141.94603528         0.        ]\n",
      " [-11159598.80186039    129142.1018407          0.        ]\n",
      " [ 11159597.0783198    -129143.7092372          0.        ]]\n",
      "[]\n",
      "[[  3.09872137e+04   2.96166104e+01]\n",
      " [  3.02596626e+04   3.00573456e+01]\n",
      " [  2.36011258e+04   2.32967314e+01]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ -1.05204237e+24  -9.60171481e+17]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 3\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 4\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 5\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 6\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 7\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 8\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 9\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 10\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 11\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 12\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 13\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 14\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 15\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 16\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 17\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 18\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 19\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 20\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 21\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 22\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 23\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 24\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 25\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 26\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 27\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 28\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 29\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 30\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 31\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 32\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 33\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 34\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 35\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 36\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 37\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 38\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 39\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 40\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 41\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 42\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 43\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 44\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 45\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 46\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 47\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 48\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n",
      "EPOCH 49\n",
      "TARGET: \n",
      "[ 0.  1.]\n",
      "WEIGHTS:\n",
      "[[ nan  nan   0.]\n",
      " [ nan  nan   0.]\n",
      " [ nan  nan   0.]]\n",
      "[]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "SOFTMAX_LAYER\n",
      "[nan, nan]\n",
      "LOSS PER INSTANCE: nan\n",
      "DERIVATIVE LOSS: nan\n",
      "DERIVATIVE ACTIVATIONS:\n",
      "[ nan  nan]\n",
      "LAYER: 0\n",
      "UPDATING WEIGHTS\n",
      "LOSS: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:61: RuntimeWarning: overflow encountered in exp\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:63: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "class TanHNN(NN):\n",
    "    \n",
    "    def _activate(self, x):\n",
    "        \"\"\"\n",
    "        Override with TanH\n",
    "        \"\"\"\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def _derivative_activation(self, x):\n",
    "        \"\"\"\n",
    "        Override with Derivative TanH (1 - tanH squared)\n",
    "        \"\"\"\n",
    "        return 1 - x*x\n",
    "    \n",
    "\"\"\"\n",
    "Run the same test with sigmoid\n",
    "\"\"\"\n",
    "MLP = TanHNN(2, 1, 2, 2)\n",
    "xor_inputs = [[1, 1], [-1, 1], [-1, -1], [1, -1]]\n",
    "xor_labels = ([1, 0, 1, 0])\n",
    "MLP.train(xor_inputs, xor_labels, epochs=50, lr=.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def neighborhood2int(n):\n",
    "    \"\"\"\n",
    "    For mapping neighborhoods into integer labels\n",
    "    \"\"\"\n",
    "    return 1 if n == \"Blmngtn\" else 0\n",
    "\n",
    "df = pd.DataFrame.from_csv(\"./data/housing_date_train_2_features.csv\")\n",
    "housing_df = df.loc[df['Neighborhood'].isin([\"BrDale\", \"Blmngtn\"])]\n",
    "\n",
    "# Grab X and its labels as a single matrix\n",
    "dataset = housing_df[[\"LotArea\", \"SalePrice\", \"Neighborhood\"]].as_matrix()\n",
    "\n",
    "# 'shuffle' the matrix to randomize the position of each sample in it\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "# Not sure why shuffling changed the type, but need to \n",
    "# change the type of each scalar in X for some numpy methods to work\n",
    "X = dataset[:, :2].astype('int64')\n",
    "# Encode the labels\n",
    "y = [neighborhood2int(s) for s in dataset[:, 2]]\n",
    "\n",
    "# Find the length of 80% of the dataset for training data\n",
    "train_length = int(X.shape[0] * 0.8)\n",
    "\n",
    "# split 80%/20% for train and test\n",
    "train_X = X[:train_length]\n",
    "test_X = X[train_length:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
