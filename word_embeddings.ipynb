{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.'], ['[', 'This', 'killing', 'of', 'a', 'respected', 'cleric', 'will', 'be', 'causing', 'us', 'trouble', 'for', 'years', 'to', 'come', '.', ']']]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Code taken from dmeeting5materials\n",
    "\n",
    "Read in the entire text of training data from the conLL data\n",
    "\"\"\"\n",
    "UDWF=1\n",
    "UDPOS=3\n",
    "data = [[]]\n",
    "for line in filter(lambda x: not x or x[0] != '#',\n",
    "                   map(lambda x:x.strip(), open(\"./conll_ud_data/en-ud-train.conllu\"))):\n",
    "    if line == '':\n",
    "        data.append([])\n",
    "    else:\n",
    "        fields = line.split()\n",
    "        data[-1].append((fields[UDWF],fields[UDPOS]))\n",
    "\n",
    "sentences = []\n",
    "for s in data:\n",
    "    sent = []\n",
    "    for wp in s:\n",
    "        sent.append(wp[0])\n",
    "    sentences.append(sent)\n",
    "    \n",
    "print(sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gensim has a wrapper over Word2Vec with lots of nice features.\n",
    "Let's use that rather than the word2vec library directly.\n",
    "\"\"\"\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "embedding_model = Word2Vec(sentences, min_count=1, iter=50, size=100, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('European', 0.7153034210205078),\n",
       " ('incitement', 0.6965106725692749),\n",
       " ('civil', 0.6940693855285645),\n",
       " ('mainly', 0.6924140453338623),\n",
       " ('commitments', 0.6844115853309631),\n",
       " ('Islands', 0.6803815364837646),\n",
       " ('hiding', 0.6751567125320435),\n",
       " ('fabrications', 0.6674426198005676),\n",
       " ('increasingly', 0.6657902002334595),\n",
       " ('Moslem', 0.6650707721710205),\n",
       " ('Andaman', 0.6650658845901489),\n",
       " ('examples', 0.658385157585144),\n",
       " ('Sunni', 0.6569175720214844),\n",
       " ('Usenet', 0.6568467617034912),\n",
       " ('Arab', 0.6557535529136658),\n",
       " ('planners', 0.6538718342781067),\n",
       " ('fallout', 0.6537030935287476),\n",
       " ('nations', 0.6519802808761597),\n",
       " ('appropriations', 0.6510022878646851),\n",
       " ('immigrants', 0.6508307456970215)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(len(embedding_model.vocab))\n",
    "embedding_model.wv.most_similar(\"terror\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<19672x19672 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 250558 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "\"\"\"\n",
    "Load from the binary file of a ppmi matrix computed with 'ppmi.py'\n",
    "over the ConLL data, with a window size of 5.\n",
    "\"\"\"\n",
    "# Load the scipy sparse matrix that was built with the script ppmi.py\n",
    "ppmi_mat = sparse.load_npz(\"./data/ppmi_mat_window_5.npz\")\n",
    "ppmi_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 19672)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pickle\n",
    "\"\"\"\n",
    "Also load the indexing dictionaries created in the ppmi.py script\n",
    "\n",
    "Perform truncated SVD over the ppmi matrix, using sklearn\n",
    "\"\"\"\n",
    "with open('./data/wtoi.pkl', 'rb') as f:\n",
    "    wtoi = pickle.load(f)\n",
    "        \n",
    "with open('./data/itow.pkl', 'rb') as f:\n",
    "    itow = pickle.load(f)\n",
    "    \n",
    "svd = TruncatedSVD(n_components=100)\n",
    "svd.fit(ppmi_mat)\n",
    "\n",
    "# 100 dimensions for 19672 words\n",
    "print(svd.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajwieme/virtual-envs/goldberg-tutorials/lib/python3.5/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terror 1.0\n",
      "intelligence 0.430401799546\n",
      "Shiite 0.398970722661\n",
      "technical 0.390630520349\n",
      "Communist 0.364976705702\n",
      "Shiites 0.360857569762\n",
      "army 0.359067203746\n",
      "rich 0.358742361813\n",
      "traders 0.357990896903\n",
      "organizations 0.345930989891\n",
      "officials 0.341767197667\n",
      "Nations 0.340519432385\n",
      "Sunni 0.337045979262\n",
      "men 0.336337437183\n",
      "23rd 0.336004461456\n",
      "trained 0.335652620588\n",
      "Allawi 0.330355228784\n",
      "Board 0.330268246303\n",
      "Iraqi 0.327811314349\n",
      "Republican 0.325112620153\n",
      "happiness 0.324619057105\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "Let's checkout the similarity between vectors\n",
    "\"\"\"\n",
    "\n",
    "def cosine_sim(X, Y):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between 2 dense vectors\n",
    "    \n",
    "    np.linalg.norm is the L2 norm\n",
    "    \"\"\"\n",
    "    return np.dot(X, Y) / (np.linalg.norm(X) * np.linalg.norm(Y))\n",
    "\n",
    "def print_n_sims(vec, n):\n",
    "    \"\"\"\n",
    "    Print the n most similar words to the given vector,\n",
    "    with their cosine similarities\n",
    "    \"\"\"\n",
    "    # Get a list of tuples (word, sim with terror) where sim is not NaN, based on the cosine_sim function\n",
    "    sims = [(k, cosine_sim(vec, svd.components_[:, wtoi[k]])) for k in wtoi.keys() if not np.isnan(cosine_sim(vec, svd.components_[:, wtoi[k]]))]\n",
    "    # Loop over them, sorted\n",
    "    for word, sim in sorted(sims, key=lambda tup: tup[1], reverse=True)[:n + 1]:\n",
    "        print(word, sim)\n",
    "\n",
    "# Do the same test as with W2V\n",
    "terror = svd.components_[:, wtoi[\"terror\"]]\n",
    "print_n_sims(terror, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Next let's implement a nerual language model\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goldberg-tutorials",
   "language": "python",
   "name": "goldberg-tutorials"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
